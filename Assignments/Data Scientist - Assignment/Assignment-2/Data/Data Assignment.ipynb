{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 1**\n",
    "**ML: P-Prog: Model Fitting**\n",
    "\n",
    "**a.\tRead csv file(TestParam.csv)**\n",
    "\n",
    "**b.\tCreate test/training sample set**\n",
    "\n",
    "**c.\tTrain model on training data**\n",
    "\n",
    "**d.\tPredict label for test data ( Label is outcome)**\n",
    "\n",
    "**e.\tShow accuracy of model.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('TestParam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#df=pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>param0</th>\n",
       "      <th>param1</th>\n",
       "      <th>param2</th>\n",
       "      <th>param3</th>\n",
       "      <th>param4</th>\n",
       "      <th>param5</th>\n",
       "      <th>param6</th>\n",
       "      <th>param7</th>\n",
       "      <th>param8</th>\n",
       "      <th>...</th>\n",
       "      <th>param774</th>\n",
       "      <th>param775</th>\n",
       "      <th>param776</th>\n",
       "      <th>param777</th>\n",
       "      <th>param778</th>\n",
       "      <th>param779</th>\n",
       "      <th>param780</th>\n",
       "      <th>param781</th>\n",
       "      <th>param782</th>\n",
       "      <th>param783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  param0  param1  param2  param3  param4  param5  param6  param7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   param8  ...  param774  param775  param776  param777  param778  param779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   param780  param781  param782  param783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label       0\n",
       "param0      0\n",
       "param1      0\n",
       "param2      0\n",
       "param3      0\n",
       "           ..\n",
       "param779    0\n",
       "param780    0\n",
       "param781    0\n",
       "param782    0\n",
       "param783    0\n",
       "Length: 785, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>param12</th>\n",
       "      <th>param13</th>\n",
       "      <th>param14</th>\n",
       "      <th>param15</th>\n",
       "      <th>param32</th>\n",
       "      <th>param33</th>\n",
       "      <th>param34</th>\n",
       "      <th>param35</th>\n",
       "      <th>param36</th>\n",
       "      <th>...</th>\n",
       "      <th>param770</th>\n",
       "      <th>param771</th>\n",
       "      <th>param772</th>\n",
       "      <th>param773</th>\n",
       "      <th>param774</th>\n",
       "      <th>param775</th>\n",
       "      <th>param776</th>\n",
       "      <th>param777</th>\n",
       "      <th>param778</th>\n",
       "      <th>param779</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 709 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  param12  param13  param14  param15  param32  param33  param34  \\\n",
       "0          1        0        0        0        0        0        0        0   \n",
       "1          0        0        0        0        0        0        0        0   \n",
       "2          1        0        0        0        0        0        0        0   \n",
       "3          4        0        0        0        0        0        0        0   \n",
       "4          0        0        0        0        0        0        0        0   \n",
       "...      ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "41995      0        0        0        0        0        0        0        0   \n",
       "41996      1        0        0        0        0        0        0        0   \n",
       "41997      7        0        0        0        0        0        0        0   \n",
       "41998      6        0        0        0        0        0        0        0   \n",
       "41999      9        0        0        0        0        0        0        0   \n",
       "\n",
       "       param35  param36  ...  param770  param771  param772  param773  \\\n",
       "0            0        0  ...         0         0         0         0   \n",
       "1            0        0  ...         0         0         0         0   \n",
       "2            0        0  ...         0         0         0         0   \n",
       "3            0        0  ...         0         0         0         0   \n",
       "4            0        0  ...         0         0         0         0   \n",
       "...        ...      ...  ...       ...       ...       ...       ...   \n",
       "41995        0        0  ...         0         0         0         0   \n",
       "41996        0        0  ...         0         0         0         0   \n",
       "41997        0        0  ...         0         0         0         0   \n",
       "41998        0        0  ...         0         0         0         0   \n",
       "41999        0        0  ...         0         0         0         0   \n",
       "\n",
       "       param774  param775  param776  param777  param778  param779  \n",
       "0             0         0         0         0         0         0  \n",
       "1             0         0         0         0         0         0  \n",
       "2             0         0         0         0         0         0  \n",
       "3             0         0         0         0         0         0  \n",
       "4             0         0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0         0  \n",
       "41996         0         0         0         0         0         0  \n",
       "41997         0         0         0         0         0         0  \n",
       "41998         0         0         0         0         0         0  \n",
       "41999         0         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 709 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, (df**2).sum() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>param0</th>\n",
       "      <th>param1</th>\n",
       "      <th>param2</th>\n",
       "      <th>param3</th>\n",
       "      <th>param4</th>\n",
       "      <th>param5</th>\n",
       "      <th>param6</th>\n",
       "      <th>param7</th>\n",
       "      <th>param8</th>\n",
       "      <th>...</th>\n",
       "      <th>param774</th>\n",
       "      <th>param775</th>\n",
       "      <th>param776</th>\n",
       "      <th>param777</th>\n",
       "      <th>param778</th>\n",
       "      <th>param779</th>\n",
       "      <th>param780</th>\n",
       "      <th>param781</th>\n",
       "      <th>param782</th>\n",
       "      <th>param783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  param0  param1  param2  param3  param4  param5  param6  param7  \\\n",
       "0        1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "1        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2        1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "3        4.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "4        NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995    NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "41996    1.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "41997    7.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "41998    6.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "41999    9.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       param8  ...  param774  param775  param776  param777  param778  \\\n",
       "0         NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "1         NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "2         NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "3         NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "4         NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995     NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "41996     NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "41997     NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "41998     NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "41999     NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "       param779  param780  param781  param782  param783  \n",
       "0           NaN       NaN       NaN       NaN       NaN  \n",
       "1           NaN       NaN       NaN       NaN       NaN  \n",
       "2           NaN       NaN       NaN       NaN       NaN  \n",
       "3           NaN       NaN       NaN       NaN       NaN  \n",
       "4           NaN       NaN       NaN       NaN       NaN  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995       NaN       NaN       NaN       NaN       NaN  \n",
       "41996       NaN       NaN       NaN       NaN       NaN  \n",
       "41997       NaN       NaN       NaN       NaN       NaN  \n",
       "41998       NaN       NaN       NaN       NaN       NaN  \n",
       "41999       NaN       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[42000 rows x 785 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df=df.replace(0,np.nan)\n",
    "df.replace(0,np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['param0'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label       int64\n",
       "param0      int64\n",
       "param1      int64\n",
       "param2      int64\n",
       "param3      int64\n",
       "            ...  \n",
       "param779    int64\n",
       "param780    int64\n",
       "param781    int64\n",
       "param782    int64\n",
       "param783    int64\n",
       "Length: 785, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label       0\n",
       "param0      0\n",
       "param1      0\n",
       "param2      0\n",
       "param3      0\n",
       "           ..\n",
       "param779    0\n",
       "param780    0\n",
       "param781    0\n",
       "param782    0\n",
       "param783    0\n",
       "Length: 785, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(axis='columns',how='all',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of        label  param0  param1  param2  param3  param4  param5  param6  param7  \\\n",
       "0          1       0       0       0       0       0       0       0       0   \n",
       "1          0       0       0       0       0       0       0       0       0   \n",
       "2          1       0       0       0       0       0       0       0       0   \n",
       "3          4       0       0       0       0       0       0       0       0   \n",
       "4          0       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       param8  ...  param774  param775  param776  param777  param778  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0         0         0   \n",
       "3           0  ...         0         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       param779  param780  param781  param782  param783  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             0         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[42000 rows x 785 columns]>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label       int64\n",
       "param0      int64\n",
       "param1      int64\n",
       "param2      int64\n",
       "param3      int64\n",
       "            ...  \n",
       "param779    int64\n",
       "param780    int64\n",
       "param781    int64\n",
       "param782    int64\n",
       "param783    int64\n",
       "Length: 785, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>param0</th>\n",
       "      <th>param1</th>\n",
       "      <th>param2</th>\n",
       "      <th>param3</th>\n",
       "      <th>param4</th>\n",
       "      <th>param5</th>\n",
       "      <th>param6</th>\n",
       "      <th>param7</th>\n",
       "      <th>param8</th>\n",
       "      <th>...</th>\n",
       "      <th>param774</th>\n",
       "      <th>param775</th>\n",
       "      <th>param776</th>\n",
       "      <th>param777</th>\n",
       "      <th>param778</th>\n",
       "      <th>param779</th>\n",
       "      <th>param780</th>\n",
       "      <th>param781</th>\n",
       "      <th>param782</th>\n",
       "      <th>param783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  param0  param1  param2  param3  param4  param5  param6  param7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   param8  ...  param774  param775  param776  param777  param778  param779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   param780  param781  param782  param783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df.replace(0,np.nan, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(how='all',inplace=True,axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 709)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>param12</th>\n",
       "      <th>param13</th>\n",
       "      <th>param14</th>\n",
       "      <th>param15</th>\n",
       "      <th>param32</th>\n",
       "      <th>param33</th>\n",
       "      <th>param34</th>\n",
       "      <th>param35</th>\n",
       "      <th>param36</th>\n",
       "      <th>...</th>\n",
       "      <th>param770</th>\n",
       "      <th>param771</th>\n",
       "      <th>param772</th>\n",
       "      <th>param773</th>\n",
       "      <th>param774</th>\n",
       "      <th>param775</th>\n",
       "      <th>param776</th>\n",
       "      <th>param777</th>\n",
       "      <th>param778</th>\n",
       "      <th>param779</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 709 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  param12  param13  param14  param15  param32  param33  param34  \\\n",
       "0     1.0      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "1     NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2     1.0      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3     4.0      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4     NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "..    ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "95    9.0      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "96    1.0      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "97    2.0      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "98    NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "99    5.0      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "    param35  param36  ...  param770  param771  param772  param773  param774  \\\n",
       "0       NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "1       NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "2       NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "3       NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "4       NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "..      ...      ...  ...       ...       ...       ...       ...       ...   \n",
       "95      NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "96      NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "97      NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "98      NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "99      NaN      NaN  ...       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "    param775  param776  param777  param778  param779  \n",
       "0        NaN       NaN       NaN       NaN       NaN  \n",
       "1        NaN       NaN       NaN       NaN       NaN  \n",
       "2        NaN       NaN       NaN       NaN       NaN  \n",
       "3        NaN       NaN       NaN       NaN       NaN  \n",
       "4        NaN       NaN       NaN       NaN       NaN  \n",
       "..       ...       ...       ...       ...       ...  \n",
       "95       NaN       NaN       NaN       NaN       NaN  \n",
       "96       NaN       NaN       NaN       NaN       NaN  \n",
       "97       NaN       NaN       NaN       NaN       NaN  \n",
       "98       NaN       NaN       NaN       NaN       NaN  \n",
       "99       NaN       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[100 rows x 709 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basically its a multiclass poblem as target variable i.e label ranges from 0-9 .\n",
    "\n",
    "#Here more than 70% of data instances have  0 values, most of the columns have 0 values and very few columns have other than 0 values.\n",
    "\n",
    "## Data is quite ambigous in nature,\n",
    "\n",
    "##One approach can be removing varibales with 0 or NA values and processing data with remaining values,Since most of the varibales have 0(70%) so cannot go ahead with this approach.\n",
    "\n",
    "##All the variable have similar names, so more information is required on the above datasets.\n",
    "\n",
    "## cannot go ahead with model creation as datasets because of above mentioned reasons.\n",
    "\n",
    "## Need more meaningfull info about the nature of datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.\tML:R-Programing/Python: Fuzzy Text Match**\n",
    "\n",
    "**a.\tRead excel file. (DocumentData.xlsx)**\n",
    "\n",
    "**b.\tCreate test/training sample set**\n",
    "\n",
    "**c.\tPredict Best Match of Test data docs from Training data docs**\n",
    "\n",
    "**d.\tOutput Matched docs, Count of Matched docs and Confidence score.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import islice\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\xlrd\\xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\xlrd\\xlsx.py:312: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n",
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\xlrd\\xlsx.py:266: DeprecationWarning: This method will be removed in future versions.  Use 'tree.iter()' or 'list(tree.iter())' instead.\n",
      "  for elem in self.tree.iter() if Element_has_iter else self.tree.getiterator():\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_excel('DocumentsData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Doc Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nozzle, 220542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Value Collection 15 Inch Long x 12 Inch Wide x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8\"W x 10\"L 4mil Clear Reclosable Bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Value Collection 12 Inch Long x 9 Inch Wide x ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                           Doc Name\n",
       "0   1                                     Nozzle, 220542\n",
       "1   2  Value Collection 15 Inch Long x 12 Inch Wide x...\n",
       "2   3  Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...\n",
       "3   4               8\"W x 10\"L 4mil Clear Reclosable Bag\n",
       "4   5  Value Collection 12 Inch Long x 9 Inch Wide x ..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('id', 2), ('doc', 0), ('name', 3), ('doc name', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "cvec.fit(df) ## count vectorizer simply gives teh count of words. \n",
    "                     ## It will have all the words from stemmed data and their frequency count in each document\n",
    "list(islice(cvec.vocabulary_.items(),20))  ## will give top 20 words(appearing highest number of times) from cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvec.vocabulary_) ## total words in cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = stopwords.words('english')\n",
    "ad_stopwords = ['x', 'like','com','use','help','create','time','want','a','about','above','across','after','afterwards']\n",
    "stops.extend(ad_stopwords)\n",
    "stops[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"b'Ram\", 'friend.', \"company'\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "noise_free_words=['']\n",
    "d=['']\n",
    "def remove_noise(input_text):\n",
    "    input_text = str(input_text).encode('ascii','ignore') ## this is putting a 'b' in the start of a sentence. Don't know why\n",
    "    input_text = str(input_text).replace(',',\"\")\n",
    "    words = str(input_text).split()  ##  splitting into words\n",
    "    #print('words are: ', words)\n",
    "    pos_words = nltk.pos_tag(words)\n",
    "    #print('Post of tags are: ', pos_words)\n",
    "    noise_free_words = [i[0] for i in pos_words if i[1] in ('NN')] ## NN means common noun\n",
    "    niose_free_words = [w for w in noise_free_words if w.lower() not in stops]\n",
    "    return noise_free_words\n",
    "\n",
    "## Testing of above function\n",
    "input_text = 'Ram was always my best friend. Google is best company'\n",
    "f=remove_noise(input_text)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Doc Name</th>\n",
       "      <th>Cleaned_Doc_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nozzle, 220542</td>\n",
       "      <td>[b'Nozzle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Value Collection 15 Inch Long x 12 Inch Wide x...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...</td>\n",
       "      <td>[b'Made]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8\"W x 10\"L 4mil Clear Reclosable Bag</td>\n",
       "      <td>[b'8\"W, Bag']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Value Collection 12 Inch Long x 9 Inch Wide x ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Marafast ABHS03002016P 3/8-16 x 2\" Button Head...</td>\n",
       "      <td>[b'Marafast, x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Marafast ABHS03002416P 3/8-16 x 2-1/2\" Button ...</td>\n",
       "      <td>[b'Marafast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>3/8\"-16 X 3/4\", Button Head Socket Cap Screws,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>5/16\"-18 x 1\" ASTM F835 Hex Drive Black Oxide ...</td>\n",
       "      <td>[b'5/16\"-18, x]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>5/16\"-18 X 1\", Button Head Socket Cap Screws, ...</td>\n",
       "      <td>[b'5/16\"-18, X]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                                           Doc Name Cleaned_Doc_Name\n",
       "0     1                                     Nozzle, 220542       [b'Nozzle]\n",
       "1     2  Value Collection 15 Inch Long x 12 Inch Wide x...               []\n",
       "2     3  Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...         [b'Made]\n",
       "3     4               8\"W x 10\"L 4mil Clear Reclosable Bag    [b'8\"W, Bag']\n",
       "4     5  Value Collection 12 Inch Long x 9 Inch Wide x ...               []\n",
       "..  ...                                                ...              ...\n",
       "95   96  Marafast ABHS03002016P 3/8-16 x 2\" Button Head...  [b'Marafast, x]\n",
       "96   97  Marafast ABHS03002416P 3/8-16 x 2-1/2\" Button ...     [b'Marafast]\n",
       "97   98  3/8\"-16 X 3/4\", Button Head Socket Cap Screws,...               []\n",
       "98   99  5/16\"-18 x 1\" ASTM F835 Hex Drive Black Oxide ...  [b'5/16\"-18, x]\n",
       "99  100  5/16\"-18 X 1\", Button Head Socket Cap Screws, ...  [b'5/16\"-18, X]\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Cleaned_Doc_Name']= df['Doc Name'].apply(remove_noise)\n",
    "doc.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Doc Name</th>\n",
       "      <th>Cleaned_Doc_Name</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nozzle, 220542</td>\n",
       "      <td>[b'Nozzle]</td>\n",
       "      <td>b'Nozzle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Value Collection 15 Inch Long x 12 Inch Wide x...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...</td>\n",
       "      <td>[b'Made]</td>\n",
       "      <td>b'Made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8\"W x 10\"L 4mil Clear Reclosable Bag</td>\n",
       "      <td>[b'8\"W, Bag']</td>\n",
       "      <td>b'8\"W Bag'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Value Collection 12 Inch Long x 9 Inch Wide x ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                           Doc Name Cleaned_Doc_Name  \\\n",
       "0   1                                     Nozzle, 220542       [b'Nozzle]   \n",
       "1   2  Value Collection 15 Inch Long x 12 Inch Wide x...               []   \n",
       "2   3  Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...         [b'Made]   \n",
       "3   4               8\"W x 10\"L 4mil Clear Reclosable Bag    [b'8\"W, Bag']   \n",
       "4   5  Value Collection 12 Inch Long x 9 Inch Wide x ...               []   \n",
       "\n",
       "      stemmed  \n",
       "0    b'Nozzle  \n",
       "1              \n",
       "2      b'Made  \n",
       "3  b'8\"W Bag'  \n",
       "4              "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##try the lemmatizer on data. WE will use WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "df['stemmed'] = df.Cleaned_Doc_Name.map(lambda x: ' '.join([lemmatizer.lemmatize(y) for y in x]))\n",
    "df.stemmed.head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer(stop_words=stops, min_df=1, max_df=0.5, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('id', 3),\n",
       " ('doc', 1),\n",
       " ('name', 4),\n",
       " ('doc name', 2),\n",
       " ('cleaned_doc_name', 0),\n",
       " ('stemmed', 5)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "cvec.fit(df) ## count vectorizer simply gives teh count of words. \n",
    "                     ## It will have all the words from stemmed data and their frequency count in each document\n",
    "list(islice(cvec.vocabulary_.items(),20))  ## will give top 20 words(appearing highest number of times) from cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvec.vocabulary_) ## total words in cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In Above case we have taken all the words by providing min_df=1 which means even if a word is \n",
    "## present only once it has taken it into consideration\n",
    "\n",
    "## We can reduce this and take only meaningful words by providing some value of min_df e.g. 0.001 i.e. a word should be present\n",
    "## at least in 1% of teh documents\n",
    "\n",
    "cvec = CountVectorizer(stop_words=stops, min_df = 0.001, max_df = 0.9, ngram_range=(1,2))\n",
    "cvec.fit(df)\n",
    "len(cvec.vocabulary_) ## So we see now we have picked up 1100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (3, 5)\n",
      "nonzero count: 5\n",
      "sparsity: 33.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "cvec_counts = cvec.transform(df)\n",
    "print('sparse matrix shape:', cvec_counts.shape)\n",
    "print('nonzero count:' , cvec_counts.nnz)\n",
    "print('sparsity: %.2f%%' %(100.0 * cvec_counts.nnz/(cvec_counts.shape[0] * cvec_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can either use countvectorizer and tfidftransformer combination or tfidfvectorized alone\n",
    "## Here we will use the tfidftransformer ov countvectorizer\n",
    "\n",
    "transformer =TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(cvec_counts)\n",
    "transformed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleaned_doc_name</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc</td>\n",
       "      <td>0.192450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc name</td>\n",
       "      <td>0.192450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name</td>\n",
       "      <td>0.192450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               term    weight\n",
       "0  cleaned_doc_name  0.333333\n",
       "3                id  0.333333\n",
       "1               doc  0.192450\n",
       "2          doc name  0.192450\n",
       "4              name  0.192450"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df=pd.DataFrame({'term':cvec.get_feature_names(), 'weight':weights}) # will provide the transformed weights of each term\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Above problem is still pending as i did not had enough time as per the deadline\n",
    "\n",
    "# Basically i wanted to clean the text and get the count and frequency of words as did in the above steps,\n",
    "# but after doing that most of the information is getting lost, so it will be  better to preserve the raw form of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.\tR-Programming:Read EmployeeDataxlsx file(3 sheets)**\n",
    "\n",
    "**a.\tAdd all files**\n",
    "\n",
    "**b.\tFile has fields: Name | Dept | Salary | …**\n",
    "\n",
    "**c.\tRemove Duplicate Records based on Key (Name, Dept, Salary)**\n",
    "\n",
    "**d.\tClub data by Dept and show output as: Dept | Salary_Range | Num_Of_Employees.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**##Above task is done in R program. Please refer R script file for this task attached within mail.##**\n",
    "\n",
    "**##Expected output has been achived by using sqldf library in R##**\n",
    "\n",
    "**sqldf('select \n",
    "case\n",
    "when salary BETWEEN 0 and 2000 then \"Group 1\"\n",
    "when salary BETWEEN 2001 and 3000 then \"Group 2\"\n",
    "when salary BETWEEN 3001 and 4000 then \"Group 3\"\n",
    "when salary BETWEEN 4001 and 5000 then \"Group 4\"\n",
    "when salary BETWEEN 5001 and 6000 then \"Group 5\"\n",
    "end as salary_range,Dept,count(*) as Num_of_Employees\n",
    "from Data_without_duplicate group by salary_range')**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 3**\n",
    "\n",
    "**Text based clustering (NLP) : Perform unsupervised topic modelling of unlabeled text descriptions**\n",
    "\n",
    "**a.\tPerform preprocessing and cleaning  text data**\n",
    "\n",
    "**b.\tExecute feature engineering method to extract text features (use TF-IDF or BOW or any other appropriate method)**\n",
    "\n",
    "**c.\tRun topic modelling clustering algorithm to label text descriptions with proper category**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_excel('DocumentsData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "import spacy  ## spacy is their for lemmatization\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11004]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.set_proxy('http://proxy.example.com:3128', ('USERNAME', 'PASSWORD'))\n",
    "#nltk.download('stopwords')\n",
    "nltk.set_proxy('http://proxy.example.com:3128', ('USERNAME', 'PASSWORD'))\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.\tPerform preprocessing and cleaning  text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m spacy download en\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Doc Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Nozzle, 220542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Value Collection 15 Inch Long x 12 Inch Wide x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8\"W x 10\"L 4mil Clear Reclosable Bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Value Collection 12 Inch Long x 9 Inch Wide x ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Marafast ABHS03002016P 3/8-16 x 2\" Button Head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Marafast ABHS03002416P 3/8-16 x 2-1/2\" Button ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>3/8\"-16 X 3/4\", Button Head Socket Cap Screws,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>5/16\"-18 x 1\" ASTM F835 Hex Drive Black Oxide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>5/16\"-18 X 1\", Button Head Socket Cap Screws, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                                           Doc Name\n",
       "0     1                                     Nozzle, 220542\n",
       "1     2  Value Collection 15 Inch Long x 12 Inch Wide x...\n",
       "2     3  Made in USA 18 Inch Long x 13 Inch Wide x 4 mi...\n",
       "3     4               8\"W x 10\"L 4mil Clear Reclosable Bag\n",
       "4     5  Value Collection 12 Inch Long x 9 Inch Wide x ...\n",
       "..  ...                                                ...\n",
       "95   96  Marafast ABHS03002016P 3/8-16 x 2\" Button Head...\n",
       "96   97  Marafast ABHS03002416P 3/8-16 x 2-1/2\" Button ...\n",
       "97   98  3/8\"-16 X 3/4\", Button Head Socket Cap Screws,...\n",
       "98   99  5/16\"-18 x 1\" ASTM F835 Hex Drive Black Oxide ...\n",
       "99  100  5/16\"-18 X 1\", Button Head Socket Cap Screws, ...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df2['Doc Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove special characters\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data_words = list(sent_to_words(data))\n",
    "#print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nozzle']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "download(model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.\tExecute feature engineering method to extract text features (use TF-IDF or BOW or any other appropriate method)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=300) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    #nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    nlp=spacy.load('en_core_web_sm')\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.Run topic modelling clustering algorithm to label text descriptions with proper category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.113*\"steel\" + 0.065*\"finish\" + 0.052*\"screw\" + 0.047*\"drive\" + '\n",
      "  '0.047*\"stainless\" + 0.045*\"hex\" + 0.044*\"nut\" + 0.037*\"head\" + '\n",
      "  '0.035*\"socket\" + 0.035*\"grade\"'),\n",
      " (1,\n",
      "  '0.060*\"size\" + 0.032*\"standard\" + 0.028*\"hose\" + 0.023*\"gray\" + '\n",
      "  '0.022*\"material\" + 0.022*\"normally\" + 0.021*\"mount\" + 0.020*\"flange\" + '\n",
      "  '0.020*\"disconnect\" + 0.020*\"enclosure\"'),\n",
      " (2,\n",
      "  '0.145*\"point\" + 0.118*\"finish\" + 0.117*\"drill\" + 0.117*\"jobber\" + '\n",
      "  '0.117*\"split\" + 0.117*\"cobalt\" + 0.116*\"gold\" + 0.013*\"ltr\" + '\n",
      "  '0.002*\"carbide\" + 0.002*\"bit\"'),\n",
      " (3,\n",
      "  '0.088*\"type\" + 0.048*\"grade\" + 0.046*\"inch\" + 0.044*\"nylon\" + '\n",
      "  '0.041*\"insert\" + 0.036*\"coarse\" + 0.031*\"length\" + 0.030*\"metal\" + '\n",
      "  '0.030*\"cut\" + 0.030*\"sheet\"')]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           iterations=100,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el46526503173764897134775\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el46526503173764897134775_data = {\"mdsDat\": {\"x\": [-0.04521476165548769, -0.3244266812111704, 0.18544214016560182, 0.18419930270105622], \"y\": [-0.3025605068573759, 0.13641621235997647, -0.021567871518916905, 0.1877121660163162], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [35.73482029030291, 23.95721335406992, 22.87665979162715, 17.431306564000018]}, \"tinfo\": {\"Term\": [\"point\", \"steel\", \"drill\", \"jobber\", \"split\", \"cobalt\", \"gold\", \"finish\", \"type\", \"screw\", \"size\", \"drive\", \"stainless\", \"hex\", \"nut\", \"inch\", \"nylon\", \"grade\", \"head\", \"insert\", \"socket\", \"coarse\", \"metal\", \"standard\", \"sheet\", \"diam\", \"cut\", \"zinc\", \"length\", \"hose\", \"steel\", \"screw\", \"drive\", \"stainless\", \"hex\", \"nut\", \"head\", \"socket\", \"zinc\", \"cap\", \"alloy\", \"button\", \"phillip\", \"collection\", \"value\", \"machine\", \"pan\", \"din\", \"astm\", \"lock\", \"thread\", \"cheese\", \"phillips\", \"plate\", \"black\", \"ne\", \"washer\", \"od\", \"carbon\", \"fully\", \"class\", \"oxide\", \"metric\", \"grade\", \"finish\", \"length\", \"point\", \"drill\", \"jobber\", \"split\", \"cobalt\", \"gold\", \"ltr\", \"carbide\", \"bit\", \"rock\", \"hammer\", \"sds_plus\", \"heavy\", \"duty\", \"straw\", \"cartridge\", \"ink\", \"hp\", \"finish\", \"cyan\", \"dry\", \"vacuum\", \"soc\", \"blk\", \"photo\", \"vac\", \"caplug\", \"clean\", \"filter\", \"gallon\", \"oval\", \"oxide\", \"burr\", \"cone\", \"type\", \"inch\", \"coarse\", \"diam\", \"metal\", \"nylon\", \"sheet\", \"max\", \"rpm\", \"aluminum\", \"disc\", \"change\", \"quick\", \"attach\", \"system\", \"grit\", \"shank\", \"file\", \"straight\", \"overall\", \"cloth\", \"insert\", \"medium\", \"fine\", \"cut\", \"american\", \"blue\", \"rectangular\", \"handle\", \"brown\", \"mill\", \"length\", \"resistant\", \"grade\", \"flute\", \"hand\", \"oxide\", \"size\", \"standard\", \"normally\", \"material\", \"white\", \"glove\", \"ansi\", \"coat\", \"shell\", \"gray\", \"mount\", \"palm\", \"open\", \"full\", \"contact\", \"voltage\", \"closed\", \"pilot\", \"light\", \"series\", \"nema\", \"hose\", \"level\", \"flange\", \"volt\", \"package\", \"color\", \"width\", \"green\", \"hyflex\", \"dim\", \"disconnect\", \"enclosure\", \"sa_disconnect\", \"polyurethane\", \"male\", \"paint\"], \"Freq\": [167.0, 194.0, 135.0, 135.0, 135.0, 135.0, 133.0, 247.0, 97.0, 89.0, 50.0, 80.0, 80.0, 77.0, 76.0, 51.0, 49.0, 113.0, 64.0, 48.0, 61.0, 40.0, 33.0, 27.0, 33.0, 32.0, 35.0, 43.0, 55.0, 25.0, 193.68410804652416, 88.76465119491475, 80.15019457237055, 80.00936415043638, 77.27378992639812, 76.19094554713605, 64.17872091718174, 60.658024280977344, 42.92201925748639, 40.72370015091665, 39.1977176858812, 27.27821680275788, 30.111279424389643, 22.39735298058614, 22.39735298058614, 21.225375119504267, 25.229223640625406, 18.12554797830293, 17.08995544914137, 40.97479500917571, 40.8221056878588, 11.07019709129459, 11.07019709129459, 11.18551697866328, 38.07516788359821, 19.007879172407478, 5.728154780734853, 5.813272182845879, 5.349577952464059, 19.654666932086467, 22.6299884121989, 36.949125282332375, 22.14974932350618, 60.45161360220332, 111.14095311118295, 20.9814911495387, 167.08823405561017, 134.44303123839146, 134.42957704171982, 134.42628633374406, 134.41608856684007, 133.33267438313973, 14.953730091147559, 2.6510155548976115, 2.028697097380319, 2.008966239450909, 1.9930347727491857, 1.9930347727491857, 1.4967346573710587, 1.4354836504077366, 1.415297267228755, 1.2697556109981678, 1.1156698436049834, 1.0222377586194662, 135.89173971605504, 0.6302909527802949, 0.5986957509278529, 0.5986957509278529, 0.5958062051060563, 0.5813460903164187, 0.6076701473313372, 0.44473574840552693, 0.42058594741722277, 0.4134263879776948, 0.4134263879776948, 0.4134263879776948, 0.7352675633294666, 0.9244139790724941, 0.42267347020444546, 0.42267347020444546, 96.66246629298452, 50.452836472759884, 39.43524664265414, 31.517120600399252, 33.15396704395417, 48.20766931121477, 32.51671724989347, 21.726237339875972, 15.68511244264913, 15.120980116082674, 13.576236805142747, 13.005907874982656, 13.005907874982656, 12.990083930183461, 12.990083930183461, 12.746340967666015, 11.974811653407565, 10.489908834946144, 11.614367353715915, 16.7549297650887, 8.605910171729906, 45.3563888054918, 8.356144495122113, 6.971839822951009, 32.584814425139854, 5.643220707009262, 5.774507793431307, 5.565014414345265, 5.289385231321981, 5.237947693370002, 5.408345086280065, 34.49510722701207, 12.904741007728646, 52.891421951691505, 12.768679631561916, 11.411267482513995, 17.1803361145704, 49.943925763433256, 26.970149705947065, 18.058664611239205, 18.308636127343725, 16.331419905729597, 16.039739446325843, 15.991598873918237, 15.881068693310588, 15.385374963319052, 19.110000590914396, 17.944878087149398, 15.25090309791367, 9.337232130237267, 9.272925191983102, 9.154562796044624, 9.14343059069382, 9.131289059155987, 9.131289059155987, 9.332370997087859, 10.849431902725758, 12.87539741006955, 23.53388895968628, 14.533807628428905, 17.040475059426438, 13.853495278689705, 7.620849177424488, 15.177743706507195, 5.87801447418825, 5.315183405905341, 14.118448468900027, 16.570267789910385, 16.570267789910385, 16.570267789910385, 16.570267789910385, 14.118448468900027, 9.994753193211205, 11.369203712945708], \"Total\": [167.0, 194.0, 135.0, 135.0, 135.0, 135.0, 133.0, 247.0, 97.0, 89.0, 50.0, 80.0, 80.0, 77.0, 76.0, 51.0, 49.0, 113.0, 64.0, 48.0, 61.0, 40.0, 33.0, 27.0, 33.0, 32.0, 35.0, 43.0, 55.0, 25.0, 194.41695308872136, 89.44325019108537, 80.83040751827888, 80.70571810421512, 77.94994580319494, 76.88582814672738, 64.86353132512139, 61.334685402910324, 43.6014582097171, 41.398800239978385, 39.87287251350414, 27.956643818561613, 30.878825697614662, 23.10722792158983, 23.10722792158983, 21.898996520819757, 26.051926976110938, 18.815204548184695, 17.764121384182438, 42.68739292492228, 42.703226180314054, 11.834747383873106, 11.834747383873106, 12.09893250064408, 41.323520395376676, 20.780712156550187, 6.402470426717323, 6.514368816151254, 6.0257769880949805, 22.22599041475667, 26.28773587809764, 55.26675286382126, 32.5653641060581, 113.78517402287233, 247.53901068636273, 55.99860759145415, 167.75948616067365, 135.0764692749697, 135.06301503703753, 135.05973228797376, 135.0495250879651, 133.96639224084132, 15.588173008351767, 3.3059902511551864, 2.6623298185493165, 2.642928473159861, 2.6290570595145244, 2.6290570595145244, 2.1315833425524255, 2.0698939861041175, 2.049289545398611, 1.9034670117281292, 1.7601815261200264, 1.7121369518315857, 247.53901068636273, 1.28536313681708, 1.2325818984769794, 1.2325818984769794, 1.232026586249595, 1.218232850328164, 1.2873664794146562, 1.0789165992150078, 1.0587213039172536, 1.0470406383903428, 1.0470406383903428, 1.0470406383903428, 6.280052659335835, 55.26675286382126, 1.4922994903883668, 1.4922994903883668, 97.5186962651112, 51.493499761802056, 40.25684727047606, 32.178125793780225, 33.9834682251771, 49.62103444731924, 33.49281082034626, 22.38591063066654, 16.314457638101906, 15.750592445271353, 14.20556292479739, 13.635215841558617, 13.635215841558617, 13.619400295730514, 13.619400295730514, 13.37600115990224, 12.654760699217501, 11.12155809750861, 12.329686695384082, 17.91566953740837, 9.235428524650908, 48.676007137245115, 8.989357228247856, 7.6012582268922975, 35.67541046886838, 6.274867528048792, 6.433525386317869, 6.200551392297804, 5.920464416061037, 5.867341402921782, 6.081229120520161, 55.99860759145415, 17.980620339708953, 113.78517402287233, 21.36823210226236, 21.0743303826809, 55.26675286382126, 50.60784263638722, 27.77391501235963, 18.715266940160728, 19.05313300731952, 16.999070341683836, 16.713793273613206, 16.671204510351323, 16.56313693585141, 16.141404743913732, 20.076774262325607, 18.873096074419728, 16.06840439592466, 9.988375057829463, 9.923221837660815, 9.807025555452654, 9.796529888244596, 9.785252631928518, 9.785252631928518, 10.014691023962527, 11.648039019410827, 13.93688421700807, 25.504699287267236, 15.96143866483035, 18.763575850760322, 15.297373186419449, 8.454708896605528, 16.966845046424588, 6.601597774452067, 5.971185550724559, 15.959253598993334, 18.75125474529329, 18.75125474529329, 18.75125474529329, 18.75125474529329, 15.959253598993334, 11.796751003414194, 19.465958404145507], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.1813, -2.9615, -3.0636, -3.0654, -3.1002, -3.1143, -3.2859, -3.3423, -3.6881, -3.7407, -3.7789, -4.1414, -4.0426, -4.3386, -4.3386, -4.3923, -4.2195, -4.5502, -4.609, -3.7346, -3.7383, -5.0433, -5.0433, -5.0329, -3.808, -4.5027, -5.7021, -5.6874, -5.7705, -4.4692, -4.3283, -3.838, -4.3497, -3.3457, -2.7367, -4.4039, -1.9292, -2.1465, -2.1466, -2.1467, -2.1467, -2.1548, -4.3427, -6.0727, -6.3403, -6.3501, -6.358, -6.358, -6.6444, -6.6862, -6.7003, -6.8089, -6.9382, -7.0257, -2.1358, -7.5093, -7.5607, -7.5607, -7.5655, -7.5901, -7.5458, -7.858, -7.9138, -7.931, -7.931, -7.931, -7.3552, -7.1263, -7.9088, -7.9088, -2.4303, -3.0805, -3.3269, -3.551, -3.5004, -3.126, -3.5198, -3.923, -4.2488, -4.2854, -4.3932, -4.4361, -4.4361, -4.4373, -4.4373, -4.4563, -4.5187, -4.6511, -4.5493, -4.1828, -4.8491, -3.187, -4.8785, -5.0596, -3.5177, -5.2711, -5.2481, -5.285, -5.3358, -5.3456, -5.3136, -3.4607, -4.4439, -3.0333, -4.4545, -4.5669, -4.1578, -2.8188, -3.4349, -3.8361, -3.8223, -3.9366, -3.9546, -3.9576, -3.9645, -3.9963, -3.7795, -3.8424, -4.005, -4.4957, -4.5026, -4.5154, -4.5166, -4.518, -4.518, -4.4962, -4.3456, -4.1744, -3.5712, -4.0532, -3.8941, -4.1011, -4.6988, -4.0098, -4.9585, -5.0591, -4.0822, -3.9221, -3.9221, -3.9221, -3.9221, -4.0822, -4.4276, -4.2988], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0253, 1.0214, 1.0206, 1.0204, 1.0203, 1.02, 1.0184, 1.018, 1.0133, 1.0126, 1.012, 1.0045, 1.0039, 0.9978, 0.9978, 0.9978, 0.997, 0.9917, 0.9904, 0.9881, 0.984, 0.9623, 0.9623, 0.9505, 0.9472, 0.9399, 0.9178, 0.9152, 0.91, 0.9061, 0.8792, 0.6264, 0.6436, 0.3966, 0.2283, 0.0474, 1.4249, 1.4242, 1.4242, 1.4242, 1.4242, 1.4242, 1.3873, 1.2081, 1.1571, 1.1546, 1.1519, 1.1519, 1.0753, 1.0629, 1.0587, 1.024, 0.9729, 0.9132, 0.8292, 0.7163, 0.7068, 0.7068, 0.7024, 0.6891, 0.6782, 0.5427, 0.5057, 0.4997, 0.4997, 0.4997, -0.716, -2.6619, 0.1674, 0.1674, 1.4662, 1.4546, 1.4544, 1.4543, 1.4503, 1.4462, 1.4455, 1.4451, 1.4357, 1.4343, 1.4297, 1.4278, 1.4278, 1.4277, 1.4277, 1.4268, 1.4198, 1.4166, 1.4153, 1.4081, 1.4045, 1.4044, 1.402, 1.3886, 1.3844, 1.369, 1.367, 1.3669, 1.3623, 1.3616, 1.3578, 0.9905, 1.1434, 0.709, 0.9601, 0.8616, 0.3066, 1.7337, 1.7175, 1.7112, 1.707, 1.7068, 1.7057, 1.7053, 1.7049, 1.6989, 1.6976, 1.6965, 1.6947, 1.6795, 1.6791, 1.6781, 1.6779, 1.6777, 1.6777, 1.6763, 1.6759, 1.6677, 1.6665, 1.6532, 1.6506, 1.6478, 1.6431, 1.6355, 1.6308, 1.6305, 1.6243, 1.6233, 1.6233, 1.6233, 1.6233, 1.6243, 1.5811, 1.2091]}, \"token.table\": {\"Topic\": [1, 3, 3, 4, 1, 3, 2, 1, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 3, 1, 1, 3, 4, 4, 3, 3, 4, 2, 1, 3, 4, 3, 4, 3, 4, 2, 3, 1, 3, 4, 1, 3, 1, 3, 4, 2, 1, 2, 2, 1, 3, 4, 3, 3, 1, 2, 3, 4, 1, 3, 4, 4, 1, 3, 4, 2, 1, 3, 4, 4, 3, 2, 1, 3, 4, 3, 1, 2, 1, 1, 3, 4, 2, 3, 4, 3, 4, 2, 1, 3, 2, 1, 3, 3, 4, 4, 1, 3, 4, 2, 1, 1, 3, 4, 4, 3, 3, 3, 1, 3, 4, 3, 4, 1, 3, 3, 4, 4, 1, 1, 3, 1, 4, 1, 2, 3, 3, 4, 1, 2, 3, 4, 3, 4, 4, 1, 1, 1, 2, 4, 1, 2, 3, 4, 3, 3, 3, 4, 2, 3, 1, 3, 4, 1, 2, 4, 3, 1, 3, 4, 4, 2, 1, 2, 1, 4, 1, 3, 2, 3, 1, 3, 4, 3, 2, 1, 3, 4, 4, 1, 4, 4, 1], \"Freq\": [0.9781086122348341, 0.9523451293734226, 0.9561954851126134, 0.9597386913504321, 0.9569851293144828, 0.9545207364288508, 0.7512217254471443, 0.9195731543784804, 0.0725978806088274, 0.8208611348237924, 0.9326146458923059, 0.852174717072051, 0.6701067757784684, 0.9657811636915281, 0.9903668647964038, 0.9074436922346438, 0.8297685111610353, 0.5253571476881624, 0.9534135836982829, 0.9294663961302129, 0.8749327103199896, 0.0760811052452165, 0.03804055262260825, 0.9197514196653135, 0.9745081103684025, 0.9687792920783984, 0.9660005868433966, 0.9922285910499761, 0.9520830484146776, 0.058938476615057515, 0.8840771492258627, 0.6701067757784684, 0.9177094470806235, 0.9250068763412537, 0.08409153421284124, 0.7779902592167696, 0.9944643825771029, 0.0533297645188788, 0.0533297645188788, 0.9066059968209397, 0.9566730966917207, 0.9855294066215035, 0.0533297645188788, 0.0533297645188788, 0.9066059968209397, 0.9920306676599728, 0.9897265454452758, 0.8113051158999125, 0.4831165299833376, 0.0533297645188788, 0.0533297645188788, 0.9066059968209397, 0.8991545889815695, 0.9209001708736692, 0.4484141699210368, 0.5494083523356847, 0.053294745519387704, 0.906010673829591, 0.14039533011635413, 0.6083797638375346, 0.2339922168605902, 0.9069634990767832, 0.8998474140761461, 0.08998474140761462, 0.9572931612873241, 0.9927863083817025, 0.5273094716886332, 0.46579003332495933, 0.9463671679396132, 0.8373546521918562, 0.9718898678755058, 0.7607290198445961, 0.237255462413603, 0.5219620173099266, 0.237255462413603, 0.8445283424786743, 0.9866869517049106, 0.46913483514211, 0.9878133872524643, 0.039208460712933484, 0.039208460712933484, 0.9410030571104036, 0.5840654270852773, 0.06265957200298373, 0.8772340080417722, 0.9709963438354225, 0.019419926876708453, 0.5681232220430714, 0.06163200674084274, 0.924480101112641, 0.9921294883225728, 0.37500932439621537, 0.6071579537843487, 0.06265099412394533, 0.9397649118591799, 0.8986797474295874, 0.9604709304245862, 0.0234261202542582, 0.0234261202542582, 0.9622679958686218, 0.9589480495161016, 0.0847691029259312, 0.0847691029259312, 0.8476910292593121, 0.944726517842764, 0.9827610036940878, 0.8899412713137116, 0.9710603926985745, 0.6755643796381617, 0.245659774413877, 0.06141494360346925, 0.8222022063152131, 0.9537385879361305, 0.9143093776991229, 0.04812154619469068, 0.07175204905409462, 0.9327766377032299, 0.961781633013962, 0.9884786550645344, 0.020152743914713465, 0.9673317079062463, 0.9210408819844579, 0.9010474624644057, 0.3184686671419592, 0.1592343335709796, 0.47770300071293886, 0.9488900185674652, 0.05581705991573325, 0.66948025861351, 0.01809406104360838, 0.3075990377413424, 0.9462182669839658, 0.3596021246253802, 0.5650890529827404, 0.9335089925795226, 0.9596219129174002, 0.9715395363081262, 0.9294663961302129, 0.7767795852931351, 0.9197514196653135, 0.9091711189738781, 0.9954727677220814, 0.06265957200298373, 0.8772340080417722, 0.9534135836982829, 0.9676558777422722, 0.7230006392655097, 0.278077168948273, 0.7567363325609862, 0.9807252165485723, 0.0533297645188788, 0.0533297645188788, 0.9066059968209397, 0.9950443416340706, 0.7607290198445961, 0.9443649683581155, 0.9482597328562691, 0.02985715368482954, 0.9852860715993749, 0.9292871492895245, 0.9879891612698349, 0.8116707960370354, 0.9945432930696267, 0.9921536029279682, 0.9912556616706659, 0.9721351846862342, 0.9978553666123392, 0.9732607402337718, 0.4879739918867776, 0.9545207364288508, 0.9601148125642266, 0.023417434452786014, 0.023417434452786014, 0.9946810582485528, 0.8113051158999125, 0.9520830484146776, 0.06537070043422685, 0.9151898060791759, 0.9186926496084704, 0.9371382607192021, 0.9412279423755314, 0.9088708832306889, 0.986205548290973], \"Term\": [\"alloy\", \"aluminum\", \"american\", \"ansi\", \"astm\", \"attach\", \"bit\", \"black\", \"black\", \"blk\", \"blue\", \"brown\", \"burr\", \"button\", \"cap\", \"carbide\", \"carbon\", \"cartridge\", \"change\", \"cheese\", \"class\", \"class\", \"class\", \"closed\", \"cloth\", \"coarse\", \"coat\", \"cobalt\", \"collection\", \"color\", \"color\", \"cone\", \"contact\", \"cut\", \"cut\", \"cyan\", \"diam\", \"dim\", \"dim\", \"dim\", \"din\", \"disc\", \"disconnect\", \"disconnect\", \"disconnect\", \"drill\", \"drive\", \"dry\", \"duty\", \"enclosure\", \"enclosure\", \"enclosure\", \"file\", \"fine\", \"finish\", \"finish\", \"flange\", \"flange\", \"flute\", \"flute\", \"flute\", \"full\", \"fully\", \"fully\", \"glove\", \"gold\", \"grade\", \"grade\", \"gray\", \"green\", \"grit\", \"hammer\", \"hand\", \"hand\", \"hand\", \"handle\", \"head\", \"heavy\", \"hex\", \"hose\", \"hose\", \"hose\", \"hp\", \"hyflex\", \"hyflex\", \"inch\", \"inch\", \"ink\", \"insert\", \"insert\", \"jobber\", \"length\", \"length\", \"level\", \"level\", \"light\", \"lock\", \"lock\", \"lock\", \"ltr\", \"machine\", \"male\", \"male\", \"male\", \"material\", \"max\", \"medium\", \"metal\", \"metric\", \"metric\", \"metric\", \"mill\", \"mount\", \"ne\", \"ne\", \"nema\", \"nema\", \"normally\", \"nut\", \"nylon\", \"nylon\", \"od\", \"open\", \"oval\", \"oval\", \"oval\", \"overall\", \"overall\", \"oxide\", \"oxide\", \"oxide\", \"package\", \"paint\", \"paint\", \"palm\", \"pan\", \"phillip\", \"phillips\", \"photo\", \"pilot\", \"plate\", \"point\", \"polyurethane\", \"polyurethane\", \"quick\", \"rectangular\", \"resistant\", \"resistant\", \"rock\", \"rpm\", \"sa_disconnect\", \"sa_disconnect\", \"sa_disconnect\", \"screw\", \"sds_plus\", \"series\", \"shank\", \"sheet\", \"sheet\", \"shell\", \"size\", \"soc\", \"socket\", \"split\", \"stainless\", \"standard\", \"steel\", \"straight\", \"straw\", \"system\", \"thread\", \"thread\", \"thread\", \"type\", \"vacuum\", \"value\", \"volt\", \"volt\", \"voltage\", \"washer\", \"white\", \"width\", \"zinc\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 4, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el46526503173764897134775\", ldavis_el46526503173764897134775_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el46526503173764897134775\", ldavis_el46526503173764897134775_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.2.2/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el46526503173764897134775\", ldavis_el46526503173764897134775_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.045215 -0.302561       1        1  35.734820\n",
       "2     -0.324427  0.136416       2        1  23.957213\n",
       "3      0.185442 -0.021568       3        1  22.876660\n",
       "1      0.184199  0.187712       4        1  17.431307, topic_info=              Term        Freq       Total Category  logprob  loglift\n",
       "159          point  167.000000  167.000000  Default  30.0000  30.0000\n",
       "100          steel  194.000000  194.000000  Default  29.0000  29.0000\n",
       "220          drill  135.000000  135.000000  Default  28.0000  28.0000\n",
       "223         jobber  135.000000  135.000000  Default  27.0000  27.0000\n",
       "137          split  135.000000  135.000000  Default  26.0000  26.0000\n",
       "..             ...         ...         ...      ...      ...      ...\n",
       "594      enclosure   16.570268   18.751255   Topic4  -3.9221   1.6233\n",
       "595  sa_disconnect   16.570268   18.751255   Topic4  -3.9221   1.6233\n",
       "607   polyurethane   14.118448   15.959254   Topic4  -4.0822   1.6243\n",
       "575           male    9.994753   11.796751   Topic4  -4.4276   1.5811\n",
       "299          paint   11.369204   19.465958   Topic4  -4.2988   1.2091\n",
       "\n",
       "[174 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "166       1  0.978109     alloy\n",
       "35        3  0.952345  aluminum\n",
       "307       3  0.956195  american\n",
       "66        4  0.959739      ansi\n",
       "269       1  0.956985      astm\n",
       "...     ...       ...       ...\n",
       "85        4  0.918693   voltage\n",
       "138       1  0.937138    washer\n",
       "132       4  0.941228     white\n",
       "341       4  0.908871     width\n",
       "120       1  0.986206      zinc\n",
       "\n",
       "[183 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 4, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10),sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment 4**\n",
    "\n",
    "**4.\tDeep learning : Build a neural net model for binary classification (Data :- deep_learning.csv)**\n",
    "\n",
    "**a.\tCreate train/test set and construct a model (using Keras or any other appropriate library)**\n",
    "\n",
    "**b.\tPredict labels for new data (test data)**\n",
    "\n",
    "**c.\tEvaluate your model and show relevant performance metrics (like precision, recall etc.)**\n",
    "\n",
    "**d.\tFine tune your model using appropriate optimization parameters and attempt improvement in model performance**\n",
    "\n",
    "**e.\tAnd save the model object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('deep_learning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>glucose concentration</th>\n",
       "      <th>blood pressure</th>\n",
       "      <th>Triceps thickness</th>\n",
       "      <th>insulin level</th>\n",
       "      <th>Body mass index</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age</th>\n",
       "      <th>Diabetes result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of times pregnant  glucose concentration  blood pressure  \\\n",
       "0                         6                    148              72   \n",
       "1                         1                     85              66   \n",
       "2                         8                    183              64   \n",
       "3                         1                     89              66   \n",
       "4                         0                    137              40   \n",
       "\n",
       "   Triceps thickness  insulin level  Body mass index  \\\n",
       "0                 35              0             33.6   \n",
       "1                 29              0             26.6   \n",
       "2                  0              0             23.3   \n",
       "3                 23             94             28.1   \n",
       "4                 35            168             43.1   \n",
       "\n",
       "   Diabetes pedigree function  Age  Diabetes result  \n",
       "0                       0.627   50                1  \n",
       "1                       0.351   31                0  \n",
       "2                       0.672   32                1  \n",
       "3                       0.167   21                0  \n",
       "4                       2.288   33                1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of times pregnant        int64\n",
       "glucose concentration           int64\n",
       "blood pressure                  int64\n",
       "Triceps thickness               int64\n",
       "insulin level                   int64\n",
       "Body mass index               float64\n",
       "Diabetes pedigree function    float64\n",
       "Age                             int64\n",
       "Diabetes result                 int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of      Number of times pregnant  glucose concentration  blood pressure  \\\n",
       "0                           6                    148              72   \n",
       "1                           1                     85              66   \n",
       "2                           8                    183              64   \n",
       "3                           1                     89              66   \n",
       "4                           0                    137              40   \n",
       "..                        ...                    ...             ...   \n",
       "763                        10                    101              76   \n",
       "764                         2                    122              70   \n",
       "765                         5                    121              72   \n",
       "766                         1                    126              60   \n",
       "767                         1                     93              70   \n",
       "\n",
       "     Triceps thickness  insulin level  Body mass index  \\\n",
       "0                   35              0             33.6   \n",
       "1                   29              0             26.6   \n",
       "2                    0              0             23.3   \n",
       "3                   23             94             28.1   \n",
       "4                   35            168             43.1   \n",
       "..                 ...            ...              ...   \n",
       "763                 48            180             32.9   \n",
       "764                 27              0             36.8   \n",
       "765                 23            112             26.2   \n",
       "766                  0              0             30.1   \n",
       "767                 31              0             30.4   \n",
       "\n",
       "     Diabetes pedigree function  Age  Diabetes result  \n",
       "0                         0.627   50                1  \n",
       "1                         0.351   31                0  \n",
       "2                         0.672   32                1  \n",
       "3                         0.167   21                0  \n",
       "4                         2.288   33                1  \n",
       "..                          ...  ...              ...  \n",
       "763                       0.171   63                0  \n",
       "764                       0.340   27                0  \n",
       "765                       0.245   30                0  \n",
       "766                       0.349   47                1  \n",
       "767                       0.315   23                0  \n",
       "\n",
       "[768 rows x 9 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Number of times pregnant      0\n",
       "glucose concentration         0\n",
       "blood pressure                0\n",
       "Triceps thickness             0\n",
       "insulin level                 0\n",
       "Body mass index               0\n",
       "Diabetes pedigree function    0\n",
       "Age                           0\n",
       "Diabetes result               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of      Number of times pregnant  glucose concentration  blood pressure  \\\n",
       "0                           6                    148              72   \n",
       "1                           1                     85              66   \n",
       "2                           8                    183              64   \n",
       "3                           1                     89              66   \n",
       "4                           0                    137              40   \n",
       "..                        ...                    ...             ...   \n",
       "763                        10                    101              76   \n",
       "764                         2                    122              70   \n",
       "765                         5                    121              72   \n",
       "766                         1                    126              60   \n",
       "767                         1                     93              70   \n",
       "\n",
       "     Triceps thickness  insulin level  Body mass index  \\\n",
       "0                   35              0             33.6   \n",
       "1                   29              0             26.6   \n",
       "2                    0              0             23.3   \n",
       "3                   23             94             28.1   \n",
       "4                   35            168             43.1   \n",
       "..                 ...            ...              ...   \n",
       "763                 48            180             32.9   \n",
       "764                 27              0             36.8   \n",
       "765                 23            112             26.2   \n",
       "766                  0              0             30.1   \n",
       "767                 31              0             30.4   \n",
       "\n",
       "     Diabetes pedigree function  Age  Diabetes result  \n",
       "0                         0.627   50                1  \n",
       "1                         0.351   31                0  \n",
       "2                         0.672   32                1  \n",
       "3                         0.167   21                0  \n",
       "4                         2.288   33                1  \n",
       "..                          ...  ...              ...  \n",
       "763                       0.171   63                0  \n",
       "764                       0.340   27                0  \n",
       "765                       0.245   30                0  \n",
       "766                       0.349   47                1  \n",
       "767                       0.315   23                0  \n",
       "\n",
       "[768 rows x 9 columns]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the data, mark zero values as NaN and drop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of times pregnant</th>\n",
       "      <th>glucose concentration</th>\n",
       "      <th>blood pressure</th>\n",
       "      <th>Triceps thickness</th>\n",
       "      <th>insulin level</th>\n",
       "      <th>Body mass index</th>\n",
       "      <th>Diabetes pedigree function</th>\n",
       "      <th>Age</th>\n",
       "      <th>Diabetes result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>733.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.153420</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>12.382158</td>\n",
       "      <td>10.476982</td>\n",
       "      <td>118.775855</td>\n",
       "      <td>6.924988</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Number of times pregnant  glucose concentration  blood pressure  \\\n",
       "count                768.000000             763.000000      733.000000   \n",
       "mean                   3.845052             121.686763       72.405184   \n",
       "std                    3.369578              30.535641       12.382158   \n",
       "min                    0.000000              44.000000       24.000000   \n",
       "25%                    1.000000              99.000000       64.000000   \n",
       "50%                    3.000000             117.000000       72.000000   \n",
       "75%                    6.000000             141.000000       80.000000   \n",
       "max                   17.000000             199.000000      122.000000   \n",
       "\n",
       "       Triceps thickness  insulin level  Body mass index  \\\n",
       "count         541.000000     394.000000       757.000000   \n",
       "mean           29.153420     155.548223        32.457464   \n",
       "std            10.476982     118.775855         6.924988   \n",
       "min             7.000000      14.000000        18.200000   \n",
       "25%            22.000000      76.250000        27.500000   \n",
       "50%            29.000000     125.000000        32.300000   \n",
       "75%            36.000000     190.000000        36.600000   \n",
       "max            99.000000     846.000000        67.100000   \n",
       "\n",
       "       Diabetes pedigree function         Age  Diabetes result  \n",
       "count                  768.000000  768.000000       768.000000  \n",
       "mean                     0.471876   33.240885         0.348958  \n",
       "std                      0.331329   11.760232         0.476951  \n",
       "min                      0.078000   21.000000         0.000000  \n",
       "25%                      0.243750   24.000000         0.000000  \n",
       "50%                      0.372500   29.000000         0.000000  \n",
       "75%                      0.626250   41.000000         1.000000  \n",
       "max                      2.420000   81.000000         1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['glucose concentration', 'blood pressure', 'Triceps thickness', 'insulin level', 'Body mass index']\n",
    "\n",
    "for col in columns:\n",
    "    df[col].replace(0, np.NaN, inplace=True)\n",
    "    \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop rows with missing values in case if there are any**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 9)\n"
     ]
    }
   ],
   "source": [
    "dataset = df.values\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. Create train/test set and construct a model (using Keras or any other appropriate library)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split into input (X) and an output (Y)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X =df[['Number of times pregnant','glucose concentration','blood pressure','Triceps thickness','insulin level','Body mass index','Age','Diabetes pedigree function']]\n",
    "#Y =df['Diabetes result'].astype(int)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.   ,  89.   ,  66.   , ...,  28.1  ,   0.167,  21.   ],\n",
       "       [  0.   , 137.   ,  40.   , ...,  43.1  ,   2.288,  33.   ],\n",
       "       [  3.   ,  78.   ,  50.   , ...,  31.   ,   0.248,  26.   ],\n",
       "       ...,\n",
       "       [  2.   ,  88.   ,  58.   , ...,  28.4  ,   0.766,  22.   ],\n",
       "       [ 10.   , 101.   ,  76.   , ...,  32.9  ,   0.171,  63.   ],\n",
       "       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 8)\n",
      "(392,)\n",
      "[0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "      <td>3.920000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.021726e-17</td>\n",
       "      <td>3.129583e-17</td>\n",
       "      <td>-4.641624e-16</td>\n",
       "      <td>1.042250e-16</td>\n",
       "      <td>6.485742e-17</td>\n",
       "      <td>1.543550e-16</td>\n",
       "      <td>3.880116e-17</td>\n",
       "      <td>1.028089e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001278e+00</td>\n",
       "      <td>1.001278e+00</td>\n",
       "      <td>1.001278e+00</td>\n",
       "      <td>1.001278e+00</td>\n",
       "      <td>1.001278e+00</td>\n",
       "      <td>1.001278e+00</td>\n",
       "      <td>1.001278e+00</td>\n",
       "      <td>1.001278e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.029213e+00</td>\n",
       "      <td>-2.161731e+00</td>\n",
       "      <td>-3.739001e+00</td>\n",
       "      <td>-2.108484e+00</td>\n",
       "      <td>-1.196867e+00</td>\n",
       "      <td>-2.120941e+00</td>\n",
       "      <td>-1.269525e+00</td>\n",
       "      <td>-9.682991e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.174265e-01</td>\n",
       "      <td>-7.665958e-01</td>\n",
       "      <td>-6.941640e-01</td>\n",
       "      <td>-7.755315e-01</td>\n",
       "      <td>-6.681786e-01</td>\n",
       "      <td>-6.676780e-01</td>\n",
       "      <td>-7.340909e-01</td>\n",
       "      <td>-7.719850e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.056403e-01</td>\n",
       "      <td>-1.176959e-01</td>\n",
       "      <td>-5.314565e-02</td>\n",
       "      <td>-1.384444e-02</td>\n",
       "      <td>-2.574448e-01</td>\n",
       "      <td>1.621036e-02</td>\n",
       "      <td>-2.131475e-01</td>\n",
       "      <td>-3.793569e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.297185e-01</td>\n",
       "      <td>6.609841e-01</td>\n",
       "      <td>5.878727e-01</td>\n",
       "      <td>7.478426e-01</td>\n",
       "      <td>2.859877e-01</td>\n",
       "      <td>5.718696e-01</td>\n",
       "      <td>4.751644e-01</td>\n",
       "      <td>5.040564e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.271153e+00</td>\n",
       "      <td>2.445459e+00</td>\n",
       "      <td>3.151946e+00</td>\n",
       "      <td>3.223325e+00</td>\n",
       "      <td>5.812990e+00</td>\n",
       "      <td>4.846172e+00</td>\n",
       "      <td>5.497667e+00</td>\n",
       "      <td>4.921123e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  3.920000e+02  3.920000e+02  3.920000e+02  3.920000e+02  3.920000e+02   \n",
       "mean  -4.021726e-17  3.129583e-17 -4.641624e-16  1.042250e-16  6.485742e-17   \n",
       "std    1.001278e+00  1.001278e+00  1.001278e+00  1.001278e+00  1.001278e+00   \n",
       "min   -1.029213e+00 -2.161731e+00 -3.739001e+00 -2.108484e+00 -1.196867e+00   \n",
       "25%   -7.174265e-01 -7.665958e-01 -6.941640e-01 -7.755315e-01 -6.681786e-01   \n",
       "50%   -4.056403e-01 -1.176959e-01 -5.314565e-02 -1.384444e-02 -2.574448e-01   \n",
       "75%    5.297185e-01  6.609841e-01  5.878727e-01  7.478426e-01  2.859877e-01   \n",
       "max    4.271153e+00  2.445459e+00  3.151946e+00  3.223325e+00  5.812990e+00   \n",
       "\n",
       "                  5             6             7  \n",
       "count  3.920000e+02  3.920000e+02  3.920000e+02  \n",
       "mean   1.543550e-16  3.880116e-17  1.028089e-16  \n",
       "std    1.001278e+00  1.001278e+00  1.001278e+00  \n",
       "min   -2.120941e+00 -1.269525e+00 -9.682991e-01  \n",
       "25%   -6.676780e-01 -7.340909e-01 -7.719850e-01  \n",
       "50%    1.621036e-02 -2.131475e-01 -3.793569e-01  \n",
       "75%    5.718696e-01  4.751644e-01  5.040564e-01  \n",
       "max    4.846172e+00  5.497667e+00  4.921123e+00  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_standardized = scaler.transform(X)\n",
    "\n",
    "data = pd.DataFrame(X_standardized)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras import Sequential\n",
    "#from tensorflow.keras.layers import Conv2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning : Build a neural net model for binary classification (Data :- deep_learning.csv)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] batch_size=10, epochs=10 ........................................\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 936us/step - loss: 0.6434 - accuracy: 0.6544\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4773 - accuracy: 0.7126\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4400 - accuracy: 0.7807\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3663 - accuracy: 0.8260\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3960 - accuracy: 0.8062\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4880 - accuracy: 0.7783\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3695 - accuracy: 0.8121\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4307 - accuracy: 0.8023\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4026 - accuracy: 0.8035\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3582 - accuracy: 0.8434\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.5108 - accuracy: 0.7595\n",
      "[CV] ............ batch_size=10, epochs=10, score=0.759, total=   1.1s\n",
      "[CV] batch_size=10, epochs=10 ........................................\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 936us/step - loss: 0.6778 - accuracy: 0.6858\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.6045 - accuracy: 0.8082\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.5204 - accuracy: 0.8328\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.5101 - accuracy: 0.8143\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4777 - accuracy: 0.8163\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4028 - accuracy: 0.8849\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.4413 - accuracy: 0.8324\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4417 - accuracy: 0.8377\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3981 - accuracy: 0.8609\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4074 - accuracy: 0.8430\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7270 - accuracy: 0.6329\n",
      "[CV] ............ batch_size=10, epochs=10, score=0.633, total=   1.3s\n",
      "[CV] batch_size=10, epochs=10 ........................................\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 936us/step - loss: 0.6837 - accuracy: 0.6039\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6322 - accuracy: 0.7608\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.5755 - accuracy: 0.7823\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.5310 - accuracy: 0.7898\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.5332 - accuracy: 0.7648\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4773 - accuracy: 0.8130\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4581 - accuracy: 0.7918\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.4785 - accuracy: 0.7887\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4836 - accuracy: 0.7655\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4455 - accuracy: 0.8255\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4184 - accuracy: 0.8462\n",
      "[CV] ............ batch_size=10, epochs=10, score=0.846, total=   1.1s\n",
      "[CV] batch_size=10, epochs=10 ........................................\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    3.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 839us/step - loss: 0.6471 - accuracy: 0.6651\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4917 - accuracy: 0.7544\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4692 - accuracy: 0.7823\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7934\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4346 - accuracy: 0.7772\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4790 - accuracy: 0.7737\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.4568 - accuracy: 0.7752\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3972 - accuracy: 0.8200\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4434 - accuracy: 0.8066\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4135 - accuracy: 0.7980\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8462\n",
      "[CV] ............ batch_size=10, epochs=10, score=0.846, total=   1.1s\n",
      "[CV] batch_size=10, epochs=10 ........................................\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    4.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 1ms/step - loss: 0.6806 - accuracy: 0.6312\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.6107 - accuracy: 0.7741\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.5498 - accuracy: 0.7909\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.5048 - accuracy: 0.8125\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.5160 - accuracy: 0.7965\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4846 - accuracy: 0.8104\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4605 - accuracy: 0.8242\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4723 - accuracy: 0.7830\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4527 - accuracy: 0.8115\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.4216 - accuracy: 0.8296\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.8462\n",
      "[CV] ............ batch_size=10, epochs=10, score=0.846, total=   1.1s\n",
      "[CV] batch_size=10, epochs=50 ........................................\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 807us/step - loss: 0.6472 - accuracy: 0.7027\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4692 - accuracy: 0.7664\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.4447 - accuracy: 0.7840\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3612 - accuracy: 0.8284\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4241 - accuracy: 0.7898\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4189 - accuracy: 0.8093\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4513 - accuracy: 0.7975\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3891 - accuracy: 0.8140\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4106 - accuracy: 0.8043\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3921 - accuracy: 0.8219\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3134 - accuracy: 0.8596\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3753 - accuracy: 0.8421\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3533 - accuracy: 0.8245\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3792 - accuracy: 0.8285\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3664 - accuracy: 0.8348\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3492 - accuracy: 0.8317\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3546 - accuracy: 0.8363\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3347 - accuracy: 0.8635\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3514 - accuracy: 0.8423\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3350 - accuracy: 0.8315\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2741 - accuracy: 0.8609\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3125 - accuracy: 0.8636\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3556 - accuracy: 0.8295\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3407 - accuracy: 0.8425\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3301 - accuracy: 0.8570\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3279 - accuracy: 0.8351\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3069 - accuracy: 0.8494\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3474 - accuracy: 0.8550\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2643 - accuracy: 0.8690\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3200 - accuracy: 0.8572\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2791 - accuracy: 0.8681\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3098 - accuracy: 0.8622\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3019 - accuracy: 0.8551\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.8844\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2968 - accuracy: 0.8815\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3084 - accuracy: 0.8527\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.8664\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3006 - accuracy: 0.8707\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3427 - accuracy: 0.8454\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3445 - accuracy: 0.8472\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3235 - accuracy: 0.8540\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2687 - accuracy: 0.8679\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2736 - accuracy: 0.8808\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2677 - accuracy: 0.8923\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2942 - accuracy: 0.8637\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2704 - accuracy: 0.8747\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3102 - accuracy: 0.8476\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2915 - accuracy: 0.8598\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2892 - accuracy: 0.8556\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2408 - accuracy: 0.8915\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.6440 - accuracy: 0.7089\n",
      "[CV] ............ batch_size=10, epochs=50, score=0.709, total=   2.5s\n",
      "[CV] batch_size=10, epochs=50 ........................................\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    8.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 871us/step - loss: 0.6398 - accuracy: 0.6710\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4229 - accuracy: 0.7966\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3264 - accuracy: 0.8603\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4078 - accuracy: 0.7943\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4298 - accuracy: 0.7895\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3861 - accuracy: 0.8377\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3635 - accuracy: 0.8297\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3845 - accuracy: 0.8302\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3513 - accuracy: 0.8520\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8582\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3235 - accuracy: 0.8526\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3959 - accuracy: 0.8403\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3253 - accuracy: 0.8696\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.90 - 0s 742us/step - loss: 0.3520 - accuracy: 0.8680\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3157 - accuracy: 0.8588\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3667 - accuracy: 0.8282\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3231 - accuracy: 0.8749\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3018 - accuracy: 0.8596\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2771 - accuracy: 0.8800\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3472 - accuracy: 0.8470\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3317 - accuracy: 0.8512\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.2787 - accuracy: 0.8856\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2767 - accuracy: 0.8830\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3135 - accuracy: 0.8774\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8500\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2853 - accuracy: 0.8886\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3121 - accuracy: 0.8535\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.3057 - accuracy: 0.8699\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2646 - accuracy: 0.8943\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2975 - accuracy: 0.8637\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2751 - accuracy: 0.8954\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2724 - accuracy: 0.8755\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2711 - accuracy: 0.8662\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3427 - accuracy: 0.8391\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2850 - accuracy: 0.8740\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2497 - accuracy: 0.8980\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2903 - accuracy: 0.8740\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2778 - accuracy: 0.8879\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2810 - accuracy: 0.8858\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2585 - accuracy: 0.8902\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2825 - accuracy: 0.8822\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2878 - accuracy: 0.8548\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2715 - accuracy: 0.8745\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2791 - accuracy: 0.8585\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2241 - accuracy: 0.8912\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2449 - accuracy: 0.8783\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3135 - accuracy: 0.8451\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2416 - accuracy: 0.8913\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2751 - accuracy: 0.8905\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2783 - accuracy: 0.8804\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.3745 - accuracy: 0.6329\n",
      "[CV] ............ batch_size=10, epochs=50, score=0.633, total=   2.3s\n",
      "[CV] batch_size=10, epochs=50 ........................................\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   10.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 807us/step - loss: 0.6809 - accuracy: 0.6336\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.5285 - accuracy: 0.7826\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4582 - accuracy: 0.7799\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4251 - accuracy: 0.8127\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4081 - accuracy: 0.8132\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4565 - accuracy: 0.7951\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.3913 - accuracy: 0.8232\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4138 - accuracy: 0.7932\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3991 - accuracy: 0.8229\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4176 - accuracy: 0.8562\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4075 - accuracy: 0.8132\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4134 - accuracy: 0.7897\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4570 - accuracy: 0.8071\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3201 - accuracy: 0.8641\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3479 - accuracy: 0.8445\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3810 - accuracy: 0.8102\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8273\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3407 - accuracy: 0.8587\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6693 - accuracy: 0.80 - 0s 903us/step - loss: 0.3968 - accuracy: 0.8494\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8305\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3556 - accuracy: 0.8219\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3245 - accuracy: 0.8597\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3207 - accuracy: 0.8466\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3523 - accuracy: 0.8575\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3821 - accuracy: 0.8521\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8391\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3353 - accuracy: 0.8603\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4038 - accuracy: 0.8525\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4177 - accuracy: 0.8401\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3077 - accuracy: 0.8886\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2954 - accuracy: 0.8550\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3458 - accuracy: 0.8330\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3234 - accuracy: 0.8511\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3274 - accuracy: 0.8484\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3488 - accuracy: 0.8405\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3199 - accuracy: 0.8860\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3286 - accuracy: 0.8711\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3488 - accuracy: 0.8485\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.3056 - accuracy: 0.8772\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3223 - accuracy: 0.8751\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3765 - accuracy: 0.8564\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3038 - accuracy: 0.8828\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3844 - accuracy: 0.8358\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2790 - accuracy: 0.9157\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3565 - accuracy: 0.8641\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2811 - accuracy: 0.8821\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2900 - accuracy: 0.8891\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3002 - accuracy: 0.8938\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3432 - accuracy: 0.8656\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8764\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.4629 - accuracy: 0.8077\n",
      "[CV] ............ batch_size=10, epochs=50, score=0.808, total=   2.3s\n",
      "[CV] batch_size=10, epochs=50 ........................................\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   12.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 968us/step - loss: 0.6689 - accuracy: 0.6362\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.4945 - accuracy: 0.7728\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4915 - accuracy: 0.7592\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4516 - accuracy: 0.7814\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4490 - accuracy: 0.7630\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4338 - accuracy: 0.8143\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.4263 - accuracy: 0.8077\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4742 - accuracy: 0.7473\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4523 - accuracy: 0.7940\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4623 - accuracy: 0.7887\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.7744\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3935 - accuracy: 0.7806\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4022 - accuracy: 0.7948\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3932 - accuracy: 0.8129\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.4382 - accuracy: 0.7804\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4208 - accuracy: 0.7816\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3660 - accuracy: 0.8131\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4320 - accuracy: 0.7983\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4360 - accuracy: 0.7548\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3864 - accuracy: 0.8154\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3958 - accuracy: 0.7899\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4135 - accuracy: 0.8070\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4154 - accuracy: 0.7874\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4175 - accuracy: 0.8018\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3822 - accuracy: 0.8250\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3937 - accuracy: 0.8090\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3572 - accuracy: 0.8511\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3554 - accuracy: 0.8395\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3825 - accuracy: 0.8117\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4029 - accuracy: 0.8095\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3748 - accuracy: 0.8363\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4107 - accuracy: 0.7976\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3657 - accuracy: 0.8349\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3417 - accuracy: 0.8438\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4195 - accuracy: 0.7922\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3507 - accuracy: 0.8139\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3418 - accuracy: 0.8433\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3490 - accuracy: 0.8485\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3678 - accuracy: 0.8128\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3412 - accuracy: 0.8312\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3724 - accuracy: 0.8325\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3607 - accuracy: 0.8538\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3691 - accuracy: 0.8440\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3645 - accuracy: 0.8343\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.3244 - accuracy: 0.8679\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3542 - accuracy: 0.8238\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3395 - accuracy: 0.8392\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3476 - accuracy: 0.8565\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3341 - accuracy: 0.8607\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3592 - accuracy: 0.8365\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8462\n",
      "[CV] ............ batch_size=10, epochs=50, score=0.846, total=   2.3s\n",
      "[CV] batch_size=10, epochs=50 ........................................\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   15.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 903us/step - loss: 0.6441 - accuracy: 0.6906\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4894 - accuracy: 0.6782\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4733 - accuracy: 0.7819\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4539 - accuracy: 0.7907\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4526 - accuracy: 0.7656\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4235 - accuracy: 0.8234\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4470 - accuracy: 0.8071\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.5084 - accuracy: 0.7764\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4029 - accuracy: 0.8073\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4422 - accuracy: 0.7956\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4081 - accuracy: 0.8131\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4154 - accuracy: 0.8226\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3693 - accuracy: 0.8468\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3758 - accuracy: 0.8569\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4559 - accuracy: 0.7653\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3927 - accuracy: 0.8149\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3640 - accuracy: 0.8311\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3723 - accuracy: 0.8383\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3788 - accuracy: 0.8139\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3869 - accuracy: 0.8245\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3587 - accuracy: 0.8078\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4755 - accuracy: 0.7561\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3576 - accuracy: 0.8578\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3390 - accuracy: 0.8598\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3838 - accuracy: 0.8404\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3059 - accuracy: 0.8873\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3736 - accuracy: 0.8270\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3577 - accuracy: 0.8437\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3389 - accuracy: 0.8407\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3463 - accuracy: 0.8382\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3349 - accuracy: 0.8463\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3582 - accuracy: 0.8631\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3477 - accuracy: 0.8651\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3257 - accuracy: 0.8774\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3495 - accuracy: 0.8411\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3287 - accuracy: 0.8650\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3272 - accuracy: 0.8658\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3520 - accuracy: 0.8496\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3709 - accuracy: 0.8305\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3245 - accuracy: 0.8670\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3180 - accuracy: 0.8569\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3242 - accuracy: 0.8674\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3693 - accuracy: 0.8321\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8503\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3286 - accuracy: 0.8626\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3476 - accuracy: 0.8513\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3433 - accuracy: 0.8633\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3137 - accuracy: 0.8471\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3221 - accuracy: 0.8819\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3173 - accuracy: 0.8670\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4159 - accuracy: 0.8333\n",
      "[CV] ............ batch_size=10, epochs=50, score=0.833, total=   2.5s\n",
      "[CV] batch_size=10, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 871us/step - loss: 0.6812 - accuracy: 0.6399\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.6196 - accuracy: 0.7538\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.5235 - accuracy: 0.7788\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.7426\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4522 - accuracy: 0.7841\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4535 - accuracy: 0.7430\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4254 - accuracy: 0.7765\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4443 - accuracy: 0.7736\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3617 - accuracy: 0.8383\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3944 - accuracy: 0.8338\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3876 - accuracy: 0.8382\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3827 - accuracy: 0.8235\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3757 - accuracy: 0.8368\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3667 - accuracy: 0.8168\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3462 - accuracy: 0.8650\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3782 - accuracy: 0.8540\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.4417 - accuracy: 0.7915\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3501 - accuracy: 0.8636\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3247 - accuracy: 0.8624\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3350 - accuracy: 0.8637\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8929\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3731 - accuracy: 0.8444\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3619 - accuracy: 0.8258\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3607 - accuracy: 0.8720\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3122 - accuracy: 0.8629\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3520 - accuracy: 0.8574\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3284 - accuracy: 0.8729\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2936 - accuracy: 0.8839\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4694 - accuracy: 0.80 - 0s 871us/step - loss: 0.3506 - accuracy: 0.8349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3266 - accuracy: 0.8808\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3385 - accuracy: 0.8392\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3099 - accuracy: 0.8642\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2967 - accuracy: 0.8906\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3568 - accuracy: 0.8113\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3426 - accuracy: 0.8579\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3238 - accuracy: 0.8732\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3507 - accuracy: 0.8361\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3267 - accuracy: 0.8614\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3117 - accuracy: 0.8736\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3072 - accuracy: 0.8800\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3111 - accuracy: 0.8820\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3292 - accuracy: 0.8638\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2984 - accuracy: 0.8756\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2853 - accuracy: 0.8992\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2773 - accuracy: 0.8957\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2844 - accuracy: 0.8912\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2427 - accuracy: 0.9125\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3078 - accuracy: 0.8528\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2989 - accuracy: 0.8738\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2792 - accuracy: 0.8869\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2872 - accuracy: 0.8711\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.2547 - accuracy: 0.9036\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2459 - accuracy: 0.9058\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2886 - accuracy: 0.8886\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.8949\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2868 - accuracy: 0.8811\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2649 - accuracy: 0.8902\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2590 - accuracy: 0.8894\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2139 - accuracy: 0.9315\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2706 - accuracy: 0.8773\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2411 - accuracy: 0.9028\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2223 - accuracy: 0.9091\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.90 - 0s 807us/step - loss: 0.2440 - accuracy: 0.8905\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2714 - accuracy: 0.8739\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2382 - accuracy: 0.8893\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2289 - accuracy: 0.9022\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2685 - accuracy: 0.8762\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2602 - accuracy: 0.8927\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2530 - accuracy: 0.9025\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2427 - accuracy: 0.9006\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2475 - accuracy: 0.9052\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2567 - accuracy: 0.8982\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2267 - accuracy: 0.9016\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2025 - accuracy: 0.9237\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2192 - accuracy: 0.9207\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2401 - accuracy: 0.8845\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.1791 - accuracy: 0.9331\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2136 - accuracy: 0.9244\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2239 - accuracy: 0.9090\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2712 - accuracy: 0.9008\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2273 - accuracy: 0.9124\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2115 - accuracy: 0.9226\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.1848 - accuracy: 0.9362\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2112 - accuracy: 0.9194\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2655 - accuracy: 0.8767\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9152\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2319 - accuracy: 0.9000\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2022 - accuracy: 0.9277\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9049\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2029 - accuracy: 0.9139\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.1843 - accuracy: 0.9450\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2284 - accuracy: 0.8962\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2010 - accuracy: 0.9438\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1895 - accuracy: 0.9296\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2227 - accuracy: 0.9059\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2113 - accuracy: 0.9104\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2075 - accuracy: 0.9245\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2220 - accuracy: 0.9139\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.1953 - accuracy: 0.9208\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.1825 - accuracy: 0.9365\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.2152 - accuracy: 0.7595\n",
      "[CV] ........... batch_size=10, epochs=100, score=0.759, total=   3.8s\n",
      "[CV] batch_size=10, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 774us/step - loss: 0.6313 - accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4763 - accuracy: 0.6939\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4112 - accuracy: 0.7160\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4537 - accuracy: 0.8109\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4184 - accuracy: 0.8090\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3787 - accuracy: 0.8517\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3728 - accuracy: 0.8294\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3522 - accuracy: 0.8522\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3861 - accuracy: 0.8345\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3847 - accuracy: 0.8244\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3671 - accuracy: 0.8477\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3373 - accuracy: 0.8622\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3820 - accuracy: 0.8117\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3604 - accuracy: 0.8324\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3394 - accuracy: 0.8537\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3117 - accuracy: 0.8824\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3414 - accuracy: 0.8433\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3161 - accuracy: 0.8931\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4009 - accuracy: 0.8276\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3536 - accuracy: 0.8576\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3533 - accuracy: 0.8493\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3174 - accuracy: 0.8825\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3491 - accuracy: 0.8540\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3307 - accuracy: 0.8537\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3833 - accuracy: 0.8426\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3812 - accuracy: 0.8373\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8894\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3483 - accuracy: 0.8550\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3197 - accuracy: 0.8426\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3205 - accuracy: 0.8584\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2852 - accuracy: 0.8938\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2929 - accuracy: 0.9055\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3559 - accuracy: 0.8429\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3437 - accuracy: 0.8467\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3500 - accuracy: 0.8579\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.6341 - accuracy: 0.70 - 0s 871us/step - loss: 0.3527 - accuracy: 0.8591\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3224 - accuracy: 0.8704\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2942 - accuracy: 0.8742\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3014 - accuracy: 0.8444\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3320 - accuracy: 0.8319\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2829 - accuracy: 0.8895\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3346 - accuracy: 0.8659\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2993 - accuracy: 0.8856\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3297 - accuracy: 0.8346\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3177 - accuracy: 0.8667\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3628 - accuracy: 0.8527\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2968 - accuracy: 0.8855\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3697 - accuracy: 0.8398\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2903 - accuracy: 0.8582\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3441 - accuracy: 0.8525\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3174 - accuracy: 0.8546\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2619 - accuracy: 0.8756\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3128 - accuracy: 0.8773\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2588 - accuracy: 0.8865\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3029 - accuracy: 0.8658\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3036 - accuracy: 0.8571\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3401 - accuracy: 0.8485\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3135 - accuracy: 0.8726\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8367\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2725 - accuracy: 0.8839\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3030 - accuracy: 0.8597\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3134 - accuracy: 0.8616\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2760 - accuracy: 0.8870\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3218 - accuracy: 0.8601\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.2967 - accuracy: 0.8495\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3061 - accuracy: 0.8522\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2693 - accuracy: 0.8956\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3431 - accuracy: 0.8529\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2948 - accuracy: 0.8844\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2925 - accuracy: 0.8827\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2687 - accuracy: 0.8788\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2925 - accuracy: 0.8751\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3009 - accuracy: 0.8727\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3114 - accuracy: 0.8669\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3084 - accuracy: 0.8811\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2531 - accuracy: 0.8926\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2911 - accuracy: 0.8639\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2873 - accuracy: 0.8660\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3106 - accuracy: 0.8802\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2948 - accuracy: 0.8779\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2616 - accuracy: 0.8953\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3158 - accuracy: 0.8987\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2394 - accuracy: 0.8847\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2865 - accuracy: 0.8684\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2707 - accuracy: 0.8725\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2863 - accuracy: 0.8786\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 839us/step - loss: 0.2825 - accuracy: 0.8843\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2940 - accuracy: 0.8738\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2883 - accuracy: 0.8614\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3503 - accuracy: 0.8325\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2798 - accuracy: 0.8814\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8868\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3235 - accuracy: 0.8800\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2632 - accuracy: 0.8880\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2241 - accuracy: 0.9134\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2657 - accuracy: 0.8885\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3494 - accuracy: 0.8611\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2620 - accuracy: 0.9025\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2774 - accuracy: 0.8943\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2537 - accuracy: 0.9187\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.8528 - accuracy: 0.6709\n",
      "[CV] ........... batch_size=10, epochs=100, score=0.671, total=   3.9s\n",
      "[CV] batch_size=10, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 903us/step - loss: 0.6763 - accuracy: 0.6157\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.5025 - accuracy: 0.7310\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4847 - accuracy: 0.7504\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4199 - accuracy: 0.7996\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4292 - accuracy: 0.8036\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4733 - accuracy: 0.7679\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3918 - accuracy: 0.8200\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4187 - accuracy: 0.8025\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.4282 - accuracy: 0.8043\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4789 - accuracy: 0.7473\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3980 - accuracy: 0.8060\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4207 - accuracy: 0.7786\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4274 - accuracy: 0.7887\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3824 - accuracy: 0.8048\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3944 - accuracy: 0.8278\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3903 - accuracy: 0.8204\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4070 - accuracy: 0.8169\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3538 - accuracy: 0.8330\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8463\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4275 - accuracy: 0.8070\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3923 - accuracy: 0.8280\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3707 - accuracy: 0.8106\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3775 - accuracy: 0.8008\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3271 - accuracy: 0.8319\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3500 - accuracy: 0.8425\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.3413 - accuracy: 0.8543\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3200 - accuracy: 0.8416\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3920 - accuracy: 0.8319\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2878 - accuracy: 0.8724\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8419\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3583 - accuracy: 0.8633\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3674 - accuracy: 0.8089\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3194 - accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3202 - accuracy: 0.8481\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3073 - accuracy: 0.8718\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3524 - accuracy: 0.8069\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.3452 - accuracy: 0.8624\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3332 - accuracy: 0.8850\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3093 - accuracy: 0.8636\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3263 - accuracy: 0.8491\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3273 - accuracy: 0.8442\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3029 - accuracy: 0.8904\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2996 - accuracy: 0.8559\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3282 - accuracy: 0.8553\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2947 - accuracy: 0.8645\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2780 - accuracy: 0.8644\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3044 - accuracy: 0.8743\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3056 - accuracy: 0.8483\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2989 - accuracy: 0.8558\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2718 - accuracy: 0.8722\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2747 - accuracy: 0.8529\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2650 - accuracy: 0.8931\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2720 - accuracy: 0.8456\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2753 - accuracy: 0.8720\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2644 - accuracy: 0.8842\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2782 - accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3552 - accuracy: 0.8439\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3390 - accuracy: 0.8291\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2746 - accuracy: 0.8725\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2525 - accuracy: 0.8986\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2938 - accuracy: 0.8581\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2540 - accuracy: 0.8867\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2401 - accuracy: 0.8917\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2690 - accuracy: 0.8712\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2642 - accuracy: 0.8587\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2407 - accuracy: 0.8972\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2644 - accuracy: 0.8848\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.8813\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2645 - accuracy: 0.8597\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2791 - accuracy: 0.8707\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3098 - accuracy: 0.8537\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.8924\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2387 - accuracy: 0.9029\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2041 - accuracy: 0.9109\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2075 - accuracy: 0.9177\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2711 - accuracy: 0.8747\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2103 - accuracy: 0.9039\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2763 - accuracy: 0.8821\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2379 - accuracy: 0.8719\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2502 - accuracy: 0.8954\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2534 - accuracy: 0.8920\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2386 - accuracy: 0.8929\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2615 - accuracy: 0.8670\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2516 - accuracy: 0.8867\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2373 - accuracy: 0.8813\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2289 - accuracy: 0.9009\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2033 - accuracy: 0.9329\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2133 - accuracy: 0.9085\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2884 - accuracy: 0.8567\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2298 - accuracy: 0.8929\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1981 - accuracy: 0.9164\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2315 - accuracy: 0.8807\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2385 - accuracy: 0.8843\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2062 - accuracy: 0.8982\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2595 - accuracy: 0.8701\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2330 - accuracy: 0.8618\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2128 - accuracy: 0.9028\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2299 - accuracy: 0.8992\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2087 - accuracy: 0.9019\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2578 - accuracy: 0.8856\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.7952 - accuracy: 0.7821\n",
      "[CV] ........... batch_size=10, epochs=100, score=0.782, total=   3.7s\n",
      "[CV] batch_size=10, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 839us/step - loss: 0.6581 - accuracy: 0.6239\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4863 - accuracy: 0.6675\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.5127 - accuracy: 0.6657\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.5358 - accuracy: 0.7453\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7449\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4735 - accuracy: 0.7553\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4744 - accuracy: 0.7957\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7912\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.4781 - accuracy: 0.7634\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4192 - accuracy: 0.8058\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4773 - accuracy: 0.7730\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4739 - accuracy: 0.7884\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4245 - accuracy: 0.8153\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3950 - accuracy: 0.8312\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4191 - accuracy: 0.8278\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4342 - accuracy: 0.8054\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.5026 - accuracy: 0.7549\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4185 - accuracy: 0.8108\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4347 - accuracy: 0.8196\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3801 - accuracy: 0.8317\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3961 - accuracy: 0.8258\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4096 - accuracy: 0.8387\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4373 - accuracy: 0.8115\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3662 - accuracy: 0.8501\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3637 - accuracy: 0.8419\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3608 - accuracy: 0.8475\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3638 - accuracy: 0.8550\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3915 - accuracy: 0.8279\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3556 - accuracy: 0.8346\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3850 - accuracy: 0.8278\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3531 - accuracy: 0.8656\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3444 - accuracy: 0.8722\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3563 - accuracy: 0.8400\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3658 - accuracy: 0.8297\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3630 - accuracy: 0.8616\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3577 - accuracy: 0.8536\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3895 - accuracy: 0.8228\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3760 - accuracy: 0.8428\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3601 - accuracy: 0.8398\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3809 - accuracy: 0.8444\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3527 - accuracy: 0.8505\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8366\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 839us/step - loss: 0.3783 - accuracy: 0.8467\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3244 - accuracy: 0.8713\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3779 - accuracy: 0.8446\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3532 - accuracy: 0.8577\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3608 - accuracy: 0.8415\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2852 - accuracy: 0.8833\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3612 - accuracy: 0.8461\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3738 - accuracy: 0.8308\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3700 - accuracy: 0.8371\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3438 - accuracy: 0.8437\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3101 - accuracy: 0.8723\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3850 - accuracy: 0.8343\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3423 - accuracy: 0.8516\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2943 - accuracy: 0.9041\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3280 - accuracy: 0.8648\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3562 - accuracy: 0.8568\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3367 - accuracy: 0.8397\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3296 - accuracy: 0.8701\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3097 - accuracy: 0.8701\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3206 - accuracy: 0.8698\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3238 - accuracy: 0.8720\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3082 - accuracy: 0.8820\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3295 - accuracy: 0.8627\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2928 - accuracy: 0.8779\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3508 - accuracy: 0.8460\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3466 - accuracy: 0.8409\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3031 - accuracy: 0.8792\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3391 - accuracy: 0.8544\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3139 - accuracy: 0.8714\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3022 - accuracy: 0.8873\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3460 - accuracy: 0.8531\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3119 - accuracy: 0.8879\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8878\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3415 - accuracy: 0.8735\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3150 - accuracy: 0.8662\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3079 - accuracy: 0.8640\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3309 - accuracy: 0.8720\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3350 - accuracy: 0.8569\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3330 - accuracy: 0.8483\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2871 - accuracy: 0.8778\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3253 - accuracy: 0.8763\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3313 - accuracy: 0.8761\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3473 - accuracy: 0.8601\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3506 - accuracy: 0.8601\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3248 - accuracy: 0.8598\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3384 - accuracy: 0.8610\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3079 - accuracy: 0.8858\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2691 - accuracy: 0.9076\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2826 - accuracy: 0.8803\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3635 - accuracy: 0.8362\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3287 - accuracy: 0.8641\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2765 - accuracy: 0.8904\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3325 - accuracy: 0.8633\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2802 - accuracy: 0.9076\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3168 - accuracy: 0.8653\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2779 - accuracy: 0.8971\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3417 - accuracy: 0.8454\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3244 - accuracy: 0.8657\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4816 - accuracy: 0.7949\n",
      "[CV] ........... batch_size=10, epochs=100, score=0.795, total=   4.1s\n",
      "[CV] batch_size=10, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 774us/step - loss: 0.6626 - accuracy: 0.7229\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.5042 - accuracy: 0.7250\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4647 - accuracy: 0.7398\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4843 - accuracy: 0.7078\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4610 - accuracy: 0.7891\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4272 - accuracy: 0.7845\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4076 - accuracy: 0.8036\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4418 - accuracy: 0.7868\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.4341 - accuracy: 0.7853\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3865 - accuracy: 0.8495\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.4277 - accuracy: 0.8094\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3752 - accuracy: 0.8402\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4121 - accuracy: 0.8181\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3605 - accuracy: 0.8435\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4330 - accuracy: 0.7725\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.8305\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3705 - accuracy: 0.8357\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3392 - accuracy: 0.8555\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3203 - accuracy: 0.8652\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3673 - accuracy: 0.8213\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3261 - accuracy: 0.8527\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4206 - accuracy: 0.8137\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3361 - accuracy: 0.8469\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3052 - accuracy: 0.8732\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3724 - accuracy: 0.8335\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3892 - accuracy: 0.8256\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3473 - accuracy: 0.8445\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3862 - accuracy: 0.8332\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3477 - accuracy: 0.8443\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3782 - accuracy: 0.8309\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3179 - accuracy: 0.8930\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3539 - accuracy: 0.8452\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3507 - accuracy: 0.8141\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3743 - accuracy: 0.8533\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3879 - accuracy: 0.8215\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3533 - accuracy: 0.8468\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3474 - accuracy: 0.8570\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.70 - 0s 871us/step - loss: 0.3402 - accuracy: 0.8626\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2990 - accuracy: 0.8879\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3399 - accuracy: 0.8745\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3327 - accuracy: 0.8520\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3318 - accuracy: 0.8635\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3218 - accuracy: 0.8606\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3220 - accuracy: 0.8711\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3530 - accuracy: 0.8542\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8962\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3318 - accuracy: 0.8824\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3049 - accuracy: 0.8914\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3303 - accuracy: 0.8738\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3148 - accuracy: 0.8863\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3160 - accuracy: 0.8883\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3464 - accuracy: 0.8966\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.3197 - accuracy: 0.8885\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3639 - accuracy: 0.8269\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3192 - accuracy: 0.8969\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2987 - accuracy: 0.8940\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2987 - accuracy: 0.8923\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2900 - accuracy: 0.8797\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3610 - accuracy: 0.8762\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3158 - accuracy: 0.8700\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3348 - accuracy: 0.8550\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3101 - accuracy: 0.8976\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3198 - accuracy: 0.8692\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3066 - accuracy: 0.9071\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2793 - accuracy: 0.8910\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.3002 - accuracy: 0.9051\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2778 - accuracy: 0.9256\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2618 - accuracy: 0.8922\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3031 - accuracy: 0.8820\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2988 - accuracy: 0.8906\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3223 - accuracy: 0.8750\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3173 - accuracy: 0.8902\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2959 - accuracy: 0.8827\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3345 - accuracy: 0.8741\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2938 - accuracy: 0.9028\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3267 - accuracy: 0.8677\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3000 - accuracy: 0.9012\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2632 - accuracy: 0.9126\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2622 - accuracy: 0.8814\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3621 - accuracy: 0.8634\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2492 - accuracy: 0.9213\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.3077 - accuracy: 0.8837\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8700\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3297 - accuracy: 0.8686\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2989 - accuracy: 0.8950\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2465 - accuracy: 0.8972\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2776 - accuracy: 0.8991\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2599 - accuracy: 0.8965\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3213 - accuracy: 0.8597\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.3049 - accuracy: 0.8706\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2520 - accuracy: 0.9152\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2653 - accuracy: 0.8948\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2701 - accuracy: 0.9038\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2396 - accuracy: 0.9156\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2514 - accuracy: 0.8790\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2866 - accuracy: 0.9050\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2751 - accuracy: 0.8911\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2952 - accuracy: 0.9011\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2589 - accuracy: 0.8971\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.2573 - accuracy: 0.9020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 857us/step - loss: 0.5722 - accuracy: 0.7949\n",
      "[CV] ........... batch_size=10, epochs=100, score=0.795, total=   3.8s\n",
      "[CV] batch_size=20, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 933us/step - loss: 0.6722 - accuracy: 0.6359\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5515 - accuracy: 0.6368\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.6502\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4604 - accuracy: 0.6551\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4320 - accuracy: 0.6870\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4821 - accuracy: 0.7913\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4017 - accuracy: 0.8292\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4151 - accuracy: 0.8247\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3748 - accuracy: 0.8483\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4236 - accuracy: 0.8188\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5715 - accuracy: 0.7722\n",
      "[CV] ............ batch_size=20, epochs=10, score=0.772, total=   1.0s\n",
      "[CV] batch_size=20, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 867us/step - loss: 0.6808 - accuracy: 0.6915\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5945 - accuracy: 0.7646\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4407 - accuracy: 0.8065\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4041 - accuracy: 0.8096\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4000 - accuracy: 0.8072\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3685 - accuracy: 0.8451\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4476 - accuracy: 0.7916\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3896 - accuracy: 0.8270\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3819 - accuracy: 0.8184\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3251 - accuracy: 0.8604\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7270 - accuracy: 0.5949\n",
      "[CV] ............ batch_size=20, epochs=10, score=0.595, total=   1.1s\n",
      "[CV] batch_size=20, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 1ms/step - loss: 0.6681 - accuracy: 0.7034\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5215 - accuracy: 0.7492\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4483 - accuracy: 0.7741\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4247 - accuracy: 0.7958\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4151 - accuracy: 0.8240\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4148 - accuracy: 0.8021\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4140 - accuracy: 0.7909\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4116 - accuracy: 0.8218\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4268 - accuracy: 0.7923\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4027 - accuracy: 0.8011\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.7949\n",
      "[CV] ............ batch_size=20, epochs=10, score=0.795, total=   1.0s\n",
      "[CV] batch_size=20, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 867us/step - loss: 0.6790 - accuracy: 0.5741\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5845 - accuracy: 0.6761\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5110 - accuracy: 0.7714\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4737 - accuracy: 0.7592\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5264 - accuracy: 0.7624\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4454 - accuracy: 0.7796\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4555 - accuracy: 0.7901\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4631 - accuracy: 0.7634\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4673 - accuracy: 0.7514\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4254 - accuracy: 0.8100\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.7949\n",
      "[CV] ............ batch_size=20, epochs=10, score=0.795, total=   1.0s\n",
      "[CV] batch_size=20, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 1ms/step - loss: 0.6659 - accuracy: 0.6489\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5264 - accuracy: 0.6824\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4895 - accuracy: 0.6410\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4588 - accuracy: 0.6865\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4573 - accuracy: 0.7806\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4675 - accuracy: 0.8050\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4592 - accuracy: 0.7739\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4341 - accuracy: 0.7974\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4445 - accuracy: 0.7828\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4376 - accuracy: 0.8095\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.4200 - accuracy: 0.8205\n",
      "[CV] ............ batch_size=20, epochs=10, score=0.821, total=   1.0s\n",
      "[CV] batch_size=20, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 933us/step - loss: 0.6764 - accuracy: 0.6002\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5455 - accuracy: 0.7121\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4918 - accuracy: 0.7338\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4292 - accuracy: 0.7845\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3796 - accuracy: 0.8168\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4017 - accuracy: 0.8032\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4203 - accuracy: 0.7888\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3974 - accuracy: 0.7964\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3952 - accuracy: 0.8137\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3761 - accuracy: 0.8422\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4165 - accuracy: 0.7890\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4047 - accuracy: 0.8230\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3942 - accuracy: 0.8209\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3764 - accuracy: 0.8181\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3770 - accuracy: 0.8255\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3879 - accuracy: 0.7989\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3821 - accuracy: 0.8170\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4269 - accuracy: 0.8155\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3730 - accuracy: 0.8332\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3936 - accuracy: 0.8223\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3342 - accuracy: 0.8651\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3547 - accuracy: 0.8491\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8485\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3633 - accuracy: 0.8346\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3403 - accuracy: 0.8531\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2948 - accuracy: 0.8751\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3239 - accuracy: 0.8648\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.8741\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.8417\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8511\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3209 - accuracy: 0.8359\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3260 - accuracy: 0.8641\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2883 - accuracy: 0.8746\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3132 - accuracy: 0.8496\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3320 - accuracy: 0.8451\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2813 - accuracy: 0.8792\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3374 - accuracy: 0.8528\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2856 - accuracy: 0.8875\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3352 - accuracy: 0.8412\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.2656 - accuracy: 0.8970\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.8625\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3222 - accuracy: 0.8682\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3276 - accuracy: 0.8334\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3655 - accuracy: 0.8348\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.2535 - accuracy: 0.9001\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2804 - accuracy: 0.8687\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.2948 - accuracy: 0.8688\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3108 - accuracy: 0.8668\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2716 - accuracy: 0.8923\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2543 - accuracy: 0.8894\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7382 - accuracy: 0.7342\n",
      "[CV] ............ batch_size=20, epochs=50, score=0.734, total=   1.9s\n",
      "[CV] batch_size=20, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 867us/step - loss: 0.6735 - accuracy: 0.5867\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5146 - accuracy: 0.7864\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3975 - accuracy: 0.8264\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4029 - accuracy: 0.8502\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4048 - accuracy: 0.8167\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3892 - accuracy: 0.8159\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3406 - accuracy: 0.8521\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3818 - accuracy: 0.8267\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3340 - accuracy: 0.8445\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2964 - accuracy: 0.8784\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3442 - accuracy: 0.8424\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3950 - accuracy: 0.8088\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4103 - accuracy: 0.8188\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3083 - accuracy: 0.8602\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3464 - accuracy: 0.8513\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2965 - accuracy: 0.8808\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3367 - accuracy: 0.8549\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3620 - accuracy: 0.8136\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3393 - accuracy: 0.8422\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3875 - accuracy: 0.8349\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3014 - accuracy: 0.8651\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3491 - accuracy: 0.8491\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3350 - accuracy: 0.8446\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3349 - accuracy: 0.8644\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3433 - accuracy: 0.8490\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3508 - accuracy: 0.8561\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8473\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3312 - accuracy: 0.8600\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3192 - accuracy: 0.8773\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2877 - accuracy: 0.8837\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3067 - accuracy: 0.8634\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2936 - accuracy: 0.8829\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3170 - accuracy: 0.8437\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2943 - accuracy: 0.8548\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2913 - accuracy: 0.8719\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3187 - accuracy: 0.8556\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3180 - accuracy: 0.8581\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2846 - accuracy: 0.8807\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3049 - accuracy: 0.8765\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3023 - accuracy: 0.8717\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3140 - accuracy: 0.8738\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2946 - accuracy: 0.8672\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2903 - accuracy: 0.8911\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3046 - accuracy: 0.8661\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2824 - accuracy: 0.8628\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8875\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2878 - accuracy: 0.8792\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 933us/step - loss: 0.2831 - accuracy: 0.8683\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2771 - accuracy: 0.8963\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.8654\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.9096 - accuracy: 0.6456\n",
      "[CV] ............ batch_size=20, epochs=50, score=0.646, total=   1.7s\n",
      "[CV] batch_size=20, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 933us/step - loss: 0.6559 - accuracy: 0.6971\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5475 - accuracy: 0.6459\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4991 - accuracy: 0.6650\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5109 - accuracy: 0.7762\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4632 - accuracy: 0.8076\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4358 - accuracy: 0.8099\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4173 - accuracy: 0.7917\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.8053\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4702 - accuracy: 0.7972\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4447 - accuracy: 0.7804\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3972 - accuracy: 0.8415\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4020 - accuracy: 0.8270\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.7832\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3621 - accuracy: 0.8517\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4004 - accuracy: 0.8000\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3996 - accuracy: 0.8226\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4074 - accuracy: 0.8227\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4101 - accuracy: 0.8187\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8189\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8197\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3915 - accuracy: 0.8418\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3602 - accuracy: 0.8403\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3693 - accuracy: 0.8363\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3244 - accuracy: 0.8518\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3942 - accuracy: 0.8255\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3765 - accuracy: 0.8035\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3465 - accuracy: 0.8363\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3772 - accuracy: 0.8462\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3527 - accuracy: 0.8394\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3558 - accuracy: 0.8387\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3300 - accuracy: 0.8723\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3219 - accuracy: 0.8820\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3183 - accuracy: 0.8797\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3320 - accuracy: 0.8679\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3345 - accuracy: 0.8788\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3355 - accuracy: 0.8500\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3068 - accuracy: 0.8776\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3790 - accuracy: 0.8585\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3597 - accuracy: 0.8629\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.8935\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3448 - accuracy: 0.8541\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3155 - accuracy: 0.8816\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2830 - accuracy: 0.8934\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3643 - accuracy: 0.8616\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3586 - accuracy: 0.8594\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2990 - accuracy: 0.8802\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3415 - accuracy: 0.8732\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3146 - accuracy: 0.8776\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3375 - accuracy: 0.8560\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3416 - accuracy: 0.8646\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.5193 - accuracy: 0.8205\n",
      "[CV] ............ batch_size=20, epochs=50, score=0.821, total=   1.7s\n",
      "[CV] batch_size=20, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 1ms/step - loss: 0.6781 - accuracy: 0.6360\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5639 - accuracy: 0.7370\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4951 - accuracy: 0.7705\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4687 - accuracy: 0.7849\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4560 - accuracy: 0.7974\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4332 - accuracy: 0.7771\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4198 - accuracy: 0.7982\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4666 - accuracy: 0.7807\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4174 - accuracy: 0.7677\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8270\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4300 - accuracy: 0.7966\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4482 - accuracy: 0.7582\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.8298\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4365 - accuracy: 0.8011\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3977 - accuracy: 0.8004\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4158 - accuracy: 0.7970\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4241 - accuracy: 0.7822\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4140 - accuracy: 0.8086\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.90 - 0s 867us/step - loss: 0.3610 - accuracy: 0.8572\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3690 - accuracy: 0.8230\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3854 - accuracy: 0.8175\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4134 - accuracy: 0.8106\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8293\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3705 - accuracy: 0.8312\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3879 - accuracy: 0.8208\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3544 - accuracy: 0.8136\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3881 - accuracy: 0.8085\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3842 - accuracy: 0.8395\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3425 - accuracy: 0.8481\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3905 - accuracy: 0.8312\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3812 - accuracy: 0.8390\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3899 - accuracy: 0.8331\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3463 - accuracy: 0.8520\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8308\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4116 - accuracy: 0.8140\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3511 - accuracy: 0.8285\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4415 - accuracy: 0.8112\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3647 - accuracy: 0.8421\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4152 - accuracy: 0.8032\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3477 - accuracy: 0.8474\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3458 - accuracy: 0.8435\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3680 - accuracy: 0.8420\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3469 - accuracy: 0.8409\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3310 - accuracy: 0.8568\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3244 - accuracy: 0.8415\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3335 - accuracy: 0.8564\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3131 - accuracy: 0.8929\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3902 - accuracy: 0.8361\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3352 - accuracy: 0.8458\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3920 - accuracy: 0.8144\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8462\n",
      "[CV] ............ batch_size=20, epochs=50, score=0.846, total=   1.9s\n",
      "[CV] batch_size=20, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 867us/step - loss: 0.6665 - accuracy: 0.6399\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5105 - accuracy: 0.6797\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5194 - accuracy: 0.7256\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4333 - accuracy: 0.7694\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4161 - accuracy: 0.7926\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4120 - accuracy: 0.7988\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4002 - accuracy: 0.7862\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4259 - accuracy: 0.7752\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4206 - accuracy: 0.8024\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.7948\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3560 - accuracy: 0.8289\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4019 - accuracy: 0.7889\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4120 - accuracy: 0.7914\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4038 - accuracy: 0.8209\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3982 - accuracy: 0.7950\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4186 - accuracy: 0.7969\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4057 - accuracy: 0.7872\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3654 - accuracy: 0.8278\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3721 - accuracy: 0.8177\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3787 - accuracy: 0.8121\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8073\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3696 - accuracy: 0.8367\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3933 - accuracy: 0.8182\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3613 - accuracy: 0.8413\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3134 - accuracy: 0.8813\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3868 - accuracy: 0.8303\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3581 - accuracy: 0.8344\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3401 - accuracy: 0.8702\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3469 - accuracy: 0.8538\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3473 - accuracy: 0.8646\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3472 - accuracy: 0.8614\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3923 - accuracy: 0.8280\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3507 - accuracy: 0.8713\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3346 - accuracy: 0.8640\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2903 - accuracy: 0.8833\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3285 - accuracy: 0.8849\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2726 - accuracy: 0.8914\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8432\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3283 - accuracy: 0.8478\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3132 - accuracy: 0.8809\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3178 - accuracy: 0.8611\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8671\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3128 - accuracy: 0.8778\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.8367\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3193 - accuracy: 0.8534\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3294 - accuracy: 0.8687\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3335 - accuracy: 0.8533\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3504 - accuracy: 0.8429\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3278 - accuracy: 0.8489\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3191 - accuracy: 0.8660\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5060 - accuracy: 0.7692\n",
      "[CV] ............ batch_size=20, epochs=50, score=0.769, total=   1.7s\n",
      "[CV] batch_size=20, epochs=100 .......................................\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 867us/step - loss: 0.6878 - accuracy: 0.5884\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6515 - accuracy: 0.7729\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6063 - accuracy: 0.7851\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.8132\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7506\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5591 - accuracy: 0.7595\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.5032 - accuracy: 0.8082\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5081 - accuracy: 0.8165\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4967 - accuracy: 0.7811\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4846 - accuracy: 0.8120\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4565 - accuracy: 0.7999\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4267 - accuracy: 0.8661\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4651 - accuracy: 0.8310\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4035 - accuracy: 0.8320\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.8269\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4047 - accuracy: 0.8325\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4454 - accuracy: 0.8301\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4029 - accuracy: 0.8348\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4061 - accuracy: 0.8438\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4442 - accuracy: 0.7942\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4302 - accuracy: 0.8332\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4445 - accuracy: 0.7905\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4222 - accuracy: 0.8364\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3416 - accuracy: 0.8935\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3731 - accuracy: 0.8542\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3728 - accuracy: 0.8808\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4274 - accuracy: 0.8103\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4301 - accuracy: 0.8269\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8228\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3826 - accuracy: 0.8557\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3618 - accuracy: 0.8543\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3750 - accuracy: 0.8358\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.90 - 0s 867us/step - loss: 0.3934 - accuracy: 0.8483\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3785 - accuracy: 0.8460\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4069 - accuracy: 0.8507\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3625 - accuracy: 0.8660\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3141 - accuracy: 0.8993\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8615\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3684 - accuracy: 0.8423\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8460\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3408 - accuracy: 0.8789\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3931 - accuracy: 0.8179\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8786\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3917 - accuracy: 0.8311\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8750\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3615 - accuracy: 0.8438\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8465\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3656 - accuracy: 0.8383\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3625 - accuracy: 0.8381\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3816 - accuracy: 0.8272\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3600 - accuracy: 0.8569\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3483 - accuracy: 0.8536\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3481 - accuracy: 0.8702\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8847\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3342 - accuracy: 0.8732\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3684 - accuracy: 0.8705\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2899 - accuracy: 0.8996\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3571 - accuracy: 0.8449\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3513 - accuracy: 0.8718\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8534\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3039 - accuracy: 0.8863\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3241 - accuracy: 0.8758\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3109 - accuracy: 0.8787\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3238 - accuracy: 0.8789\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3410 - accuracy: 0.8500\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3615 - accuracy: 0.8573\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3413 - accuracy: 0.8565\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3335 - accuracy: 0.8871\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3747 - accuracy: 0.8485\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3613 - accuracy: 0.8676\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3095 - accuracy: 0.8765\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.75 - 0s 933us/step - loss: 0.3705 - accuracy: 0.8490\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3052 - accuracy: 0.9046\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3002 - accuracy: 0.8967\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3796 - accuracy: 0.8543\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3472 - accuracy: 0.8615\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2949 - accuracy: 0.9037\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3187 - accuracy: 0.8756\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3398 - accuracy: 0.8643\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3086 - accuracy: 0.8863\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 867us/step - loss: 0.3305 - accuracy: 0.8447\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8765\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3173 - accuracy: 0.9009\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3405 - accuracy: 0.8703\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3084 - accuracy: 0.8875\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3465 - accuracy: 0.8826\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2981 - accuracy: 0.8818\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2978 - accuracy: 0.8837\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3772 - accuracy: 0.8517\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3324 - accuracy: 0.8821\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3939 - accuracy: 0.8471\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3140 - accuracy: 0.8645\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2718 - accuracy: 0.8972\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3172 - accuracy: 0.8802\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2765 - accuracy: 0.9070\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3443 - accuracy: 0.8843\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3810 - accuracy: 0.8479\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3006 - accuracy: 0.9015\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3054 - accuracy: 0.8976\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3092 - accuracy: 0.8721\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.6341 - accuracy: 0.7089\n",
      "[CV] ........... batch_size=20, epochs=100, score=0.709, total=   2.6s\n",
      "[CV] batch_size=20, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 933us/step - loss: 0.6814 - accuracy: 0.6489\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5794 - accuracy: 0.7806\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4177 - accuracy: 0.8359\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4249 - accuracy: 0.8268\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3522 - accuracy: 0.8435\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3163 - accuracy: 0.8681\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3589 - accuracy: 0.8615\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3459 - accuracy: 0.8582\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3673 - accuracy: 0.8303\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3449 - accuracy: 0.8481\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3826 - accuracy: 0.8398\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3206 - accuracy: 0.8699\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3596 - accuracy: 0.8432\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3445 - accuracy: 0.8384\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3339 - accuracy: 0.8530\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3739 - accuracy: 0.8418\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3196 - accuracy: 0.8712\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.8742\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3154 - accuracy: 0.8623\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2973 - accuracy: 0.8815\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.8734\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3193 - accuracy: 0.8839\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2988 - accuracy: 0.8887\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.8949\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3242 - accuracy: 0.8704\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3275 - accuracy: 0.8587\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3166 - accuracy: 0.8846\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3234 - accuracy: 0.8836\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2957 - accuracy: 0.8754\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2878 - accuracy: 0.8877\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8918\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2669 - accuracy: 0.9070\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8830\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.2842 - accuracy: 0.9038\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2802 - accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2890 - accuracy: 0.9161\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3440 - accuracy: 0.8837\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2900 - accuracy: 0.8983\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9081\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3195 - accuracy: 0.8855\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2823 - accuracy: 0.8926\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3278 - accuracy: 0.8833\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9199\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2910 - accuracy: 0.8902\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.9145\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2780 - accuracy: 0.8989\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.8996\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2752 - accuracy: 0.9074\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2612 - accuracy: 0.9025\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2916 - accuracy: 0.9126\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.9271\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2621 - accuracy: 0.8998\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2642 - accuracy: 0.9047\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2593 - accuracy: 0.8881\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.8898\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2238 - accuracy: 0.9174\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.8942\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9177\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3021 - accuracy: 0.8852\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2461 - accuracy: 0.9080\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2495 - accuracy: 0.9220\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2441 - accuracy: 0.9219\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2836 - accuracy: 0.8833\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2946 - accuracy: 0.8691\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.9098\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2448 - accuracy: 0.9047\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2832 - accuracy: 0.9082\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2480 - accuracy: 0.9172\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2604 - accuracy: 0.9005\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2902 - accuracy: 0.9022\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2575 - accuracy: 0.8975\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2529 - accuracy: 0.8837\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2121 - accuracy: 0.9240\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2463 - accuracy: 0.9094\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2502 - accuracy: 0.9180\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2682 - accuracy: 0.9048\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2777 - accuracy: 0.8873\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9232\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.2018 - accuracy: 0.9246\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2142 - accuracy: 0.9188\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9295\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2244 - accuracy: 0.9192\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9298\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2214 - accuracy: 0.9198\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2458 - accuracy: 0.9148\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2381 - accuracy: 0.9160\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2340 - accuracy: 0.9183\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2083 - accuracy: 0.9355\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2096 - accuracy: 0.9232\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2132 - accuracy: 0.9197\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2450 - accuracy: 0.9140\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2308 - accuracy: 0.9199\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2482 - accuracy: 0.9102\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2076 - accuracy: 0.9189\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.9103\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2038 - accuracy: 0.9385\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9167\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2129 - accuracy: 0.9196\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2290 - accuracy: 0.9095\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2108 - accuracy: 0.9279\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.2027 - accuracy: 0.6456\n",
      "[CV] ........... batch_size=20, epochs=100, score=0.646, total=   2.7s\n",
      "[CV] batch_size=20, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 1ms/step - loss: 0.6797 - accuracy: 0.5899\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5409 - accuracy: 0.6526\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4807 - accuracy: 0.7342\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4763 - accuracy: 0.7766\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4446 - accuracy: 0.7721\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4430 - accuracy: 0.7840\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.7903\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4215 - accuracy: 0.7962\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3964 - accuracy: 0.8219\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3916 - accuracy: 0.8157\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3696 - accuracy: 0.8501\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4095 - accuracy: 0.8217\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3791 - accuracy: 0.8363\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3883 - accuracy: 0.8051\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4175 - accuracy: 0.8040\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3935 - accuracy: 0.8297\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8335\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3995 - accuracy: 0.8064\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3588 - accuracy: 0.8187\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8247\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3575 - accuracy: 0.8258\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3760 - accuracy: 0.8160\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3738 - accuracy: 0.8411\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3408 - accuracy: 0.8390\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3630 - accuracy: 0.8176\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3502 - accuracy: 0.8520\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3946 - accuracy: 0.7893\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3628 - accuracy: 0.8275\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3835 - accuracy: 0.8261\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8166\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3689 - accuracy: 0.8474\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3885 - accuracy: 0.8207\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3501 - accuracy: 0.8442\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3713 - accuracy: 0.8315\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8349\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3736 - accuracy: 0.8331\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 867us/step - loss: 0.3779 - accuracy: 0.8136\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3560 - accuracy: 0.8429\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3290 - accuracy: 0.8731\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3519 - accuracy: 0.8318\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8487\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3834 - accuracy: 0.8429\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3270 - accuracy: 0.8616\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3376 - accuracy: 0.8778\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.8630\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2958 - accuracy: 0.8697\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3632 - accuracy: 0.8426\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3476 - accuracy: 0.8120\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3743 - accuracy: 0.8683\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3703 - accuracy: 0.8366\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8476\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2907 - accuracy: 0.8906\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3587 - accuracy: 0.8626\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3407 - accuracy: 0.8491\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3049 - accuracy: 0.8656\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3862 - accuracy: 0.8436\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3719 - accuracy: 0.8451\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3195 - accuracy: 0.8650\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3095 - accuracy: 0.8654\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3119 - accuracy: 0.8609\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3242 - accuracy: 0.8852\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3084 - accuracy: 0.8847\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3435 - accuracy: 0.8736\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3797 - accuracy: 0.8621\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3546 - accuracy: 0.8492\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3257 - accuracy: 0.8617\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.8771\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8546\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3653 - accuracy: 0.8562\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3184 - accuracy: 0.8775\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3130 - accuracy: 0.8777\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3243 - accuracy: 0.8949\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3244 - accuracy: 0.8814\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3593 - accuracy: 0.8583\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3184 - accuracy: 0.9068\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3176 - accuracy: 0.8724\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2892 - accuracy: 0.9024\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3318 - accuracy: 0.8860\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3294 - accuracy: 0.8816\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2861 - accuracy: 0.9002\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2805 - accuracy: 0.8949\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2681 - accuracy: 0.9044\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.8952\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.8950\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2568 - accuracy: 0.9212\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.2934 - accuracy: 0.8997\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3328 - accuracy: 0.8641\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3090 - accuracy: 0.8758\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3002 - accuracy: 0.8926\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3098 - accuracy: 0.8883\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3075 - accuracy: 0.8751\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3174 - accuracy: 0.8869\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2731 - accuracy: 0.9301\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3060 - accuracy: 0.8881\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3025 - accuracy: 0.8976\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.8830\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3127 - accuracy: 0.9022\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2943 - accuracy: 0.8922\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8874\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.9034\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4934 - accuracy: 0.8205\n",
      "[CV] ........... batch_size=20, epochs=100, score=0.821, total=   2.8s\n",
      "[CV] batch_size=20, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 933us/step - loss: 0.6872 - accuracy: 0.6607\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6557 - accuracy: 0.7529\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6070 - accuracy: 0.7813\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.6027 - accuracy: 0.7378\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5689 - accuracy: 0.7549\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5461 - accuracy: 0.7707\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5110 - accuracy: 0.7925\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5132 - accuracy: 0.7775\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4944 - accuracy: 0.8136\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4576 - accuracy: 0.8317\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4758 - accuracy: 0.8036\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4718 - accuracy: 0.7894\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4508 - accuracy: 0.7963\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4341 - accuracy: 0.8242\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.5063 - accuracy: 0.7643\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4562 - accuracy: 0.8020\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5011 - accuracy: 0.7586\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4719 - accuracy: 0.7945\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4316 - accuracy: 0.8188\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4109 - accuracy: 0.8057\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.7928\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4382 - accuracy: 0.8209\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4369 - accuracy: 0.7804\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4144 - accuracy: 0.7867\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4017 - accuracy: 0.8035\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4250 - accuracy: 0.8023\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4235 - accuracy: 0.8317\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4165 - accuracy: 0.8326\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4088 - accuracy: 0.8347\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4383 - accuracy: 0.8026\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4389 - accuracy: 0.8035\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8228\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.8372\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3894 - accuracy: 0.8430\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3800 - accuracy: 0.8513\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4332 - accuracy: 0.7913\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4118 - accuracy: 0.8175\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4102 - accuracy: 0.8448\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4211 - accuracy: 0.8011\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4309 - accuracy: 0.8056\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4417 - accuracy: 0.8087\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4482 - accuracy: 0.8051\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3966 - accuracy: 0.8204\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.8082\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3662 - accuracy: 0.8382\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3426 - accuracy: 0.8611\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4013 - accuracy: 0.8404\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3877 - accuracy: 0.8263\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4059 - accuracy: 0.8123\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3386 - accuracy: 0.8586\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3742 - accuracy: 0.8546\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3416 - accuracy: 0.8557\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4009 - accuracy: 0.8378\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3788 - accuracy: 0.8287\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3676 - accuracy: 0.8509\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3856 - accuracy: 0.8295\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3795 - accuracy: 0.8425\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3840 - accuracy: 0.8433\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4002 - accuracy: 0.8350\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4060 - accuracy: 0.8210\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3916 - accuracy: 0.8096\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3706 - accuracy: 0.8490\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.90 - 0s 867us/step - loss: 0.3929 - accuracy: 0.8366\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3688 - accuracy: 0.8651\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3654 - accuracy: 0.8434\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3591 - accuracy: 0.8549\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3549 - accuracy: 0.8494\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3922 - accuracy: 0.8295\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3808 - accuracy: 0.8365\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4232 - accuracy: 0.8111\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3530 - accuracy: 0.8412\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3737 - accuracy: 0.8552\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3916 - accuracy: 0.8344\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3402 - accuracy: 0.8578\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3754 - accuracy: 0.8244\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3891 - accuracy: 0.8392\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4202 - accuracy: 0.8268\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4457 - accuracy: 0.7891\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3997 - accuracy: 0.8257\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3351 - accuracy: 0.8692\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4018 - accuracy: 0.8242\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3447 - accuracy: 0.8498\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3605 - accuracy: 0.8570\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3829 - accuracy: 0.8265\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3619 - accuracy: 0.8343\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3705 - accuracy: 0.8339\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3790 - accuracy: 0.8535\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3997 - accuracy: 0.8341\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3357 - accuracy: 0.8451\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3519 - accuracy: 0.8549\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4186 - accuracy: 0.8209\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3926 - accuracy: 0.8070\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4015 - accuracy: 0.8288\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3255 - accuracy: 0.8710\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 867us/step - loss: 0.3766 - accuracy: 0.8234\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3036 - accuracy: 0.8782\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3141 - accuracy: 0.8676\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3556 - accuracy: 0.8564\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3414 - accuracy: 0.8685\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3712 - accuracy: 0.8488\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.8333\n",
      "[CV] ........... batch_size=20, epochs=100, score=0.833, total=   2.6s\n",
      "[CV] batch_size=20, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 867us/step - loss: 0.6888 - accuracy: 0.5726\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.6532 - accuracy: 0.7739\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6054 - accuracy: 0.7859\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5870 - accuracy: 0.7568\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.5614 - accuracy: 0.7536\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5089 - accuracy: 0.8209\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5382 - accuracy: 0.7802\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.5179 - accuracy: 0.8219\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4926 - accuracy: 0.8089\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4640 - accuracy: 0.8401\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4830 - accuracy: 0.8099\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4519 - accuracy: 0.8205\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4405 - accuracy: 0.8206\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4440 - accuracy: 0.8082\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8141\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4194 - accuracy: 0.8169\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4542 - accuracy: 0.7896\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4279 - accuracy: 0.8211\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4023 - accuracy: 0.8216\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4138 - accuracy: 0.8180\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3852 - accuracy: 0.8322\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3941 - accuracy: 0.8034\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4185 - accuracy: 0.7959\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.4083 - accuracy: 0.7965\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.7807\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8392\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3592 - accuracy: 0.8482\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8257\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3555 - accuracy: 0.8335\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3929 - accuracy: 0.8090\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.4106 - accuracy: 0.8155\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3702 - accuracy: 0.8289\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3816 - accuracy: 0.8404\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8267\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3434 - accuracy: 0.8572\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3853 - accuracy: 0.8142\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3436 - accuracy: 0.8561\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3704 - accuracy: 0.8375\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3871 - accuracy: 0.8071\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3697 - accuracy: 0.8423\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4050 - accuracy: 0.8237\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3212 - accuracy: 0.8537\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3915 - accuracy: 0.8046\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3563 - accuracy: 0.8396\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3833 - accuracy: 0.8476\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3843 - accuracy: 0.8345\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3853 - accuracy: 0.8433\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3714 - accuracy: 0.8408\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3348 - accuracy: 0.8364\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3347 - accuracy: 0.8673\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3318 - accuracy: 0.8521\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3470 - accuracy: 0.8314\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3887 - accuracy: 0.8324\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3781 - accuracy: 0.8435\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3509 - accuracy: 0.8370\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3404 - accuracy: 0.8657\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3134 - accuracy: 0.8794\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3506 - accuracy: 0.8338\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3483 - accuracy: 0.8443\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3753 - accuracy: 0.8414\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3508 - accuracy: 0.8575\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3686 - accuracy: 0.8605\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3163 - accuracy: 0.8822\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3734 - accuracy: 0.8189\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3303 - accuracy: 0.8768\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3551 - accuracy: 0.8477\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3178 - accuracy: 0.8636\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3021 - accuracy: 0.8628\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3173 - accuracy: 0.8663\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8871\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3153 - accuracy: 0.8983\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8549\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8716\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3914 - accuracy: 0.8567\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2845 - accuracy: 0.8792\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.2974 - accuracy: 0.8621\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3339 - accuracy: 0.8751\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3453 - accuracy: 0.8714\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3341 - accuracy: 0.8744\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3535 - accuracy: 0.8637\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3136 - accuracy: 0.8844\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3684 - accuracy: 0.8504\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3413 - accuracy: 0.8674\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8564\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3388 - accuracy: 0.8694\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3479 - accuracy: 0.8505\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3781 - accuracy: 0.8756\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3176 - accuracy: 0.8899\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3286 - accuracy: 0.8738\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.2912 - accuracy: 0.8975\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3374 - accuracy: 0.8714\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3426 - accuracy: 0.8652\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3099 - accuracy: 0.8893\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.2931 - accuracy: 0.8780\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3269 - accuracy: 0.8880\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.3575 - accuracy: 0.8703\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3122 - accuracy: 0.8722\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3300 - accuracy: 0.8872\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 0s 867us/step - loss: 0.3316 - accuracy: 0.8935\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 0s 800us/step - loss: 0.3121 - accuracy: 0.8795\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.8205\n",
      "[CV] ........... batch_size=20, epochs=100, score=0.821, total=   2.6s\n",
      "[CV] batch_size=40, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6897 - accuracy: 0.6377\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6563 - accuracy: 0.6927\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5872 - accuracy: 0.7479\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5141 - accuracy: 0.7719\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4719 - accuracy: 0.7742\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.7639\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4776 - accuracy: 0.7462\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.7690\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8097\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4479 - accuracy: 0.7770\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7722\n",
      "[CV] ............ batch_size=40, epochs=10, score=0.772, total=   0.9s\n",
      "[CV] batch_size=40, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6996\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.6864\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4981 - accuracy: 0.6993\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4628 - accuracy: 0.7915\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4190 - accuracy: 0.8034\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8579\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.8285\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8311\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3285 - accuracy: 0.8578\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8269\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7297 - accuracy: 0.6076\n",
      "[CV] ............ batch_size=40, epochs=10, score=0.608, total=   1.5s\n",
      "[CV] batch_size=40, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6856 - accuracy: 0.5186\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6311 - accuracy: 0.7003\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5536 - accuracy: 0.7354\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7697\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.7556\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7272\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7988\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4249 - accuracy: 0.8064\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8013\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4186 - accuracy: 0.7886\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000000025B0D8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4065 - accuracy: 0.8077\n",
      "[CV] ............ batch_size=40, epochs=10, score=0.808, total=   1.1s\n",
      "[CV] batch_size=40, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6805 - accuracy: 0.6857\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6067 - accuracy: 0.7417\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.7250\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4928 - accuracy: 0.7413\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.7555\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4671 - accuracy: 0.7453\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.7979\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4422 - accuracy: 0.7807\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.7766\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.7656\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000000025A88670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3541 - accuracy: 0.7949\n",
      "[CV] ............ batch_size=40, epochs=10, score=0.795, total=   1.1s\n",
      "[CV] batch_size=40, epochs=10 ........................................\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6773 - accuracy: 0.6733\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5812 - accuracy: 0.6850\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.6572\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.6625\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.6869\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.6970\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.6937\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4525 - accuracy: 0.7890\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.7942\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4524 - accuracy: 0.7511\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000000025DB4550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.8462\n",
      "[CV] ............ batch_size=40, epochs=10, score=0.846, total=   1.0s\n",
      "[CV] batch_size=40, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6812 - accuracy: 0.6376\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6329 - accuracy: 0.6372\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5692 - accuracy: 0.6611\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5289 - accuracy: 0.6516\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.7127\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5122 - accuracy: 0.6634\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4652 - accuracy: 0.6795\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.6691\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.7034\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.8119\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4158 - accuracy: 0.8188\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4148 - accuracy: 0.8164\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8156\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.8286\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.8536\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8543\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4241 - accuracy: 0.8245\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8328\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8254\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8660\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8596\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8313\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3640 - accuracy: 0.8495\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8526\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3434 - accuracy: 0.8636\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.8622\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8551\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8475\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8306\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3742 - accuracy: 0.8356\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.8949\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.8712\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3242 - accuracy: 0.8566\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3060 - accuracy: 0.8845\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3288 - accuracy: 0.8701\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3177 - accuracy: 0.8547\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3201 - accuracy: 0.8646\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8573\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.8741\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2897 - accuracy: 0.8871\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8883\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.8752\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.8597\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8791\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.8763\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8793\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3052 - accuracy: 0.8749\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8550\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.9003\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3424 - accuracy: 0.8517\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000000002445DD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.7864 - accuracy: 0.7215\n",
      "[CV] ............ batch_size=40, epochs=50, score=0.722, total=   1.6s\n",
      "[CV] batch_size=40, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6845 - accuracy: 0.6184\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6278 - accuracy: 0.7299\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5368 - accuracy: 0.7580\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8424\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4107 - accuracy: 0.8197\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.7944\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8278\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8229\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8517\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4197 - accuracy: 0.8058\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8056\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8599\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3632 - accuracy: 0.8429\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8422\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3470 - accuracy: 0.8392\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8415\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8697\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3696 - accuracy: 0.8415\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8396\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8534\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8454\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8672\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8610\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8624\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8649\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2987 - accuracy: 0.8747\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8778\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3169 - accuracy: 0.8612\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8565\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8634\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2989 - accuracy: 0.8876\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2719 - accuracy: 0.8689\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8561\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3417 - accuracy: 0.8618\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.8772\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8594\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.8768\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.8855\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8767\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3055 - accuracy: 0.8755\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.8885\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.9036\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.8759\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8784\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2896 - accuracy: 0.8920\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.9115\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2827 - accuracy: 0.8869\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.8802\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8840\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.9004\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000000026C82EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.8508 - accuracy: 0.6582\n",
      "[CV] ............ batch_size=40, epochs=50, score=0.658, total=   1.4s\n",
      "[CV] batch_size=40, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6804 - accuracy: 0.6336\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.6292\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.6822\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.6585\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5291 - accuracy: 0.6355\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.6517\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4759 - accuracy: 0.6783\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.7816\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.7652\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.8078\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.7985\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4243 - accuracy: 0.7896\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.7952\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4459 - accuracy: 0.7937\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4189 - accuracy: 0.8095\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.8289\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8380\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8254\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3726 - accuracy: 0.8280\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8174\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8429\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3720 - accuracy: 0.8315\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8028\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8431\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3549 - accuracy: 0.8429\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8462\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8512\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8486\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3808 - accuracy: 0.8584\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3194 - accuracy: 0.8736\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8550\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3025 - accuracy: 0.8894\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3421 - accuracy: 0.8553\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8617\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3521 - accuracy: 0.8473\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8646\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.8812\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3421 - accuracy: 0.8640\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3636 - accuracy: 0.8772\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8772\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8692\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8824\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8757\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8550\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2944 - accuracy: 0.8882\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8603\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.9035\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8797\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2864 - accuracy: 0.9077\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3106 - accuracy: 0.8802\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000000025B0D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4528 - accuracy: 0.8333\n",
      "[CV] ............ batch_size=40, epochs=50, score=0.833, total=   1.5s\n",
      "[CV] batch_size=40, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.6379\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.6816\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.6593\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4631 - accuracy: 0.6976\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.6510\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.6917\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7982\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.8080\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.7801\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.7833\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.8043\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.8191\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.8056\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.8001\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.7786\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.8547\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4096 - accuracy: 0.8498\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.8078\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.8062\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8076\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.8000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8044\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8144\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.7867\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4696 - accuracy: 0.7744\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8378\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8287\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.8128\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8261\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4035 - accuracy: 0.8136\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4152 - accuracy: 0.7968\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8057\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8532\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8228\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3449 - accuracy: 0.8676\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4071 - accuracy: 0.8262\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8507\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3918 - accuracy: 0.8315\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3771 - accuracy: 0.8581\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.8390\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8543\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.8327\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8478\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8700\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8554\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8596\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8305\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8220\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3910 - accuracy: 0.8298\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8600\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000000002677B430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3276 - accuracy: 0.8205\n",
      "[CV] ............ batch_size=40, epochs=50, score=0.821, total=   1.7s\n",
      "[CV] batch_size=40, epochs=50 ........................................\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6859 - accuracy: 0.5354\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6253 - accuracy: 0.7290\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5388 - accuracy: 0.7747\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4538 - accuracy: 0.7853\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4857 - accuracy: 0.7600\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4518 - accuracy: 0.7692\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4690 - accuracy: 0.7473\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.7547\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4352 - accuracy: 0.7538\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4137 - accuracy: 0.7802\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.7645\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8060\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4084 - accuracy: 0.7914\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3911 - accuracy: 0.8184\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8449\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4323 - accuracy: 0.7991\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3793 - accuracy: 0.8004\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3995 - accuracy: 0.8250\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4293 - accuracy: 0.7892\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4120 - accuracy: 0.7983\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.8102\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 0.8214\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.8251\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8011\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.8083\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3746 - accuracy: 0.8317\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.8294\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3817 - accuracy: 0.8191\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4090 - accuracy: 0.8060\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8065\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8257\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3582 - accuracy: 0.8233\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8397\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3734 - accuracy: 0.8291\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8123\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.7974\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8144\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3868 - accuracy: 0.7887\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8280\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3525 - accuracy: 0.8236\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8035\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4114 - accuracy: 0.8167\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3589 - accuracy: 0.8317\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.8154\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3927 - accuracy: 0.8165\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8306\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3443 - accuracy: 0.8427\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.82 - 0s 1ms/step - loss: 0.3784 - accuracy: 0.8102\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8063\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8299\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000000002445D550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8590\n",
      "[CV] ............ batch_size=40, epochs=50, score=0.859, total=   1.7s\n",
      "[CV] batch_size=40, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6830 - accuracy: 0.6113\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6261 - accuracy: 0.6664\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5542 - accuracy: 0.6470\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.6581\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.6982\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.7889\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.7806\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3865 - accuracy: 0.8302\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.7845\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.7770\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3928 - accuracy: 0.8221\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4162 - accuracy: 0.7873\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3794 - accuracy: 0.8261\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8070\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3909 - accuracy: 0.7959\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8266\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3856 - accuracy: 0.8093\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8295\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8359\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8221\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3765 - accuracy: 0.8377\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3488 - accuracy: 0.8684\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8645\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3866 - accuracy: 0.8362\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8711\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8219\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3567 - accuracy: 0.8519\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3147 - accuracy: 0.8717\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8414\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3529 - accuracy: 0.8446\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8378\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2909 - accuracy: 0.8881\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8785\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8634\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3195 - accuracy: 0.8605\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8634\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8667\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8657\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8668\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8531\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.8465\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.8900\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3183 - accuracy: 0.8723\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3539 - accuracy: 0.8623\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3248 - accuracy: 0.8663\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8775\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8636\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8842\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3467 - accuracy: 0.8665\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2811 - accuracy: 0.8829\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8532\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2951 - accuracy: 0.8845\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8457\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8578\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.8645\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8609\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3199 - accuracy: 0.8789\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3232 - accuracy: 0.8679\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3240 - accuracy: 0.8825\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3405 - accuracy: 0.8675\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8589\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8565\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8777\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3107 - accuracy: 0.8756\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.8575\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8789\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2971 - accuracy: 0.8809\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8585\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8670\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.8780\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2930 - accuracy: 0.8781\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.9000\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8604\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3008 - accuracy: 0.8842\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8816\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8933\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8609\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3314 - accuracy: 0.8651\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.8847\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8986\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8820\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.8889\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2801 - accuracy: 0.8903\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2800 - accuracy: 0.8832\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2887 - accuracy: 0.8878\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2700 - accuracy: 0.9117\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.9204\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2724 - accuracy: 0.8822\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.8923\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8820\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2663 - accuracy: 0.8934\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.9090\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8666\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9014\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2564 - accuracy: 0.8957\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.92 - 0s 1ms/step - loss: 0.2650 - accuracy: 0.9111\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2700 - accuracy: 0.8878\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.8989\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.8888\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2725 - accuracy: 0.8740\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000000263EC430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.9152 - accuracy: 0.7089\n",
      "[CV] ........... batch_size=40, epochs=100, score=0.709, total=   1.9s\n",
      "[CV] batch_size=40, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6830 - accuracy: 0.6365\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6169 - accuracy: 0.7100\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7909\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8068\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4038 - accuracy: 0.8156\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.4130 - accuracy: 0.8190\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3800 - accuracy: 0.8025\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8283\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3621 - accuracy: 0.8165\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8739\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8658\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8511\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.8424\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8270\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3064 - accuracy: 0.8921\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8596\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8840\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8650\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3735 - accuracy: 0.8478\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.8723\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.8772\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3139 - accuracy: 0.8729\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3406 - accuracy: 0.8696\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3191 - accuracy: 0.8751\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3887 - accuracy: 0.8466\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3565 - accuracy: 0.8388\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8391\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8567\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3504 - accuracy: 0.8565\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8421\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8690\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.8536\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8419\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8731\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8669\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2912 - accuracy: 0.8747\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8601\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3358 - accuracy: 0.8470\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8721\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.8659\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3027 - accuracy: 0.8633\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.8416\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8797\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8405\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8592\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8617\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8505\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3006 - accuracy: 0.8729\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8746\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.8705\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8681\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.8824\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.8807\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 0.8829\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8630\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8455\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2745 - accuracy: 0.8780\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3084 - accuracy: 0.8749\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8843\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.8854\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.8912\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.9129\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9081\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8584\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8800\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8608\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3563 - accuracy: 0.8487\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2981 - accuracy: 0.8727\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3089 - accuracy: 0.8668\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8928\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.8978\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8691\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.8796\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8653\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2911 - accuracy: 0.8777\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2839 - accuracy: 0.9000\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2797 - accuracy: 0.8819\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8762\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.8821\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3005 - accuracy: 0.8837\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8931\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3201 - accuracy: 0.8755\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8755\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8535\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8725\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2836 - accuracy: 0.8864\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8910\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8811\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2798 - accuracy: 0.8753\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.8414\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2773 - accuracy: 0.8861\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8774\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3230 - accuracy: 0.8633\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.8941\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8932\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3008 - accuracy: 0.8591\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8936\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.9074\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.9001\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8743\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000000002677AC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.7801 - accuracy: 0.7089\n",
      "[CV] ........... batch_size=40, epochs=100, score=0.709, total=   1.9s\n",
      "[CV] batch_size=40, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6870 - accuracy: 0.6569\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6461 - accuracy: 0.6578\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5801 - accuracy: 0.6687\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.6709\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4914 - accuracy: 0.6683\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5257 - accuracy: 0.6503\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.6800\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5066 - accuracy: 0.6415\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4790 - accuracy: 0.6878\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4742 - accuracy: 0.7836\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4949 - accuracy: 0.7503\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.7828\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4480 - accuracy: 0.7793\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8113\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4773 - accuracy: 0.7752\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4515 - accuracy: 0.8182\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4189 - accuracy: 0.8165\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.8335\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.8268\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4449 - accuracy: 0.8153\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8487\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3924 - accuracy: 0.8462\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3857 - accuracy: 0.8358\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8325\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8489\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4020 - accuracy: 0.8433\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8172\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8263\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8246\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.8282\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8322\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8381\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3467 - accuracy: 0.8511\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8063\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8606\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8176\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8313\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8450\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3590 - accuracy: 0.8375\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8710\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8407\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3807 - accuracy: 0.8363\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3478 - accuracy: 0.8640\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8615\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3858 - accuracy: 0.8368\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8627\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8326\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8405\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8546\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8635\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4187 - accuracy: 0.8180\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.8677\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8424\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8601\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3913 - accuracy: 0.8305\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8734\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3622 - accuracy: 0.8475\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3543 - accuracy: 0.8465\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3759 - accuracy: 0.8318\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3262 - accuracy: 0.8747\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3478 - accuracy: 0.8808\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3429 - accuracy: 0.8614\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8706\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3772 - accuracy: 0.8388\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8891\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3636 - accuracy: 0.8523\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8725\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8659\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8649\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3127 - accuracy: 0.8906\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8350\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.8503\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8429\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8631\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3060 - accuracy: 0.8767\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8782\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3081 - accuracy: 0.8830\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8716\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8808\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3263 - accuracy: 0.8796\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8558\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8664\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3261 - accuracy: 0.8617\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8835\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.8915\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8754\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8413\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8639\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3129 - accuracy: 0.8716\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3403 - accuracy: 0.8570\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3124 - accuracy: 0.8697\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8671\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8768\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3103 - accuracy: 0.8683\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8823\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8764\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8562\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8755\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3201 - accuracy: 0.8695\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000000241C2AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.8462\n",
      "[CV] ........... batch_size=40, epochs=100, score=0.846, total=   2.0s\n",
      "[CV] batch_size=40, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6901 - accuracy: 0.5891\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6719 - accuracy: 0.7020\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6361 - accuracy: 0.7425\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5796 - accuracy: 0.7511\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5396 - accuracy: 0.7555\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5085 - accuracy: 0.7678\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7587\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4057 - accuracy: 0.8233\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4473 - accuracy: 0.7846\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4188 - accuracy: 0.7886\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.7641\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.7858\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.7771\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.7578\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.7610\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8081\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4081 - accuracy: 0.8112\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.7967\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4027 - accuracy: 0.8083\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8180\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.7850\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.7919\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3892 - accuracy: 0.8073\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4272 - accuracy: 0.7980\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.7806\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8136\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8244\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.7848\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8159\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8136\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4108 - accuracy: 0.8103\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8159\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3831 - accuracy: 0.8246\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.8408\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4117 - accuracy: 0.7836\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.8169\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8081\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3730 - accuracy: 0.8430\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8189\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8377\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8044\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4133 - accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8049\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3868 - accuracy: 0.8231\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8406\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3879 - accuracy: 0.8257\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.8103\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3508 - accuracy: 0.8434\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8390\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.8604\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4069 - accuracy: 0.8313\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8260\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8514\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8569\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8580\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8359\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3724 - accuracy: 0.8560\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8388\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8418\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3764 - accuracy: 0.8545\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3969 - accuracy: 0.8315\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.3621 - accuracy: 0.87 - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8568\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3581 - accuracy: 0.8586\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8384\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8487\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8317\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8407\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3563 - accuracy: 0.8537\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8630\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3443 - accuracy: 0.8789\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3620 - accuracy: 0.8650\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3225 - accuracy: 0.8678\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8522\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8609\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3780 - accuracy: 0.8420\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8691\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8517\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3655 - accuracy: 0.8477\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8583\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3378 - accuracy: 0.8741\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8736\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3510 - accuracy: 0.8547\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8624\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8664\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.8784\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8565\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3465 - accuracy: 0.8758\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3417 - accuracy: 0.8639\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3489 - accuracy: 0.8553\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8925\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3474 - accuracy: 0.8515\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8524\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3599 - accuracy: 0.8523\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3488 - accuracy: 0.8529\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3463 - accuracy: 0.8623\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3905 - accuracy: 0.8390\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 857us/step - loss: 0.3746 - accuracy: 0.8323\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3620 - accuracy: 0.8388\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.8825\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8723\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000000025A883A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7949\n",
      "[CV] ........... batch_size=40, epochs=100, score=0.795, total=   2.0s\n",
      "[CV] batch_size=40, epochs=100 .......................................\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 1ms/step - loss: 0.6882 - accuracy: 0.6791\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6650 - accuracy: 0.6676\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.6228 - accuracy: 0.6653\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5512 - accuracy: 0.7104\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4906 - accuracy: 0.7690\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.7468\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.7706\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.7732\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7649\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.7777\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4135 - accuracy: 0.8008\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4912 - accuracy: 0.7477\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.7774\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4206 - accuracy: 0.7908\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4494 - accuracy: 0.7922\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4545 - accuracy: 0.7807\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4045 - accuracy: 0.7784\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7371\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.7725\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.7806\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.7746\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4041 - accuracy: 0.7711\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.7904\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4360 - accuracy: 0.7766\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7631\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.7983\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4469 - accuracy: 0.7613\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.7674\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4143 - accuracy: 0.7699\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4173 - accuracy: 0.7917\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.7806\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3929 - accuracy: 0.8077\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4135 - accuracy: 0.7835\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4039 - accuracy: 0.8034\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.7851\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.7492\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.7956\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4319 - accuracy: 0.7850\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4515 - accuracy: 0.7783\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4421 - accuracy: 0.7685\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3888 - accuracy: 0.8024\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3780 - accuracy: 0.7997\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.8206\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.7930\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3760 - accuracy: 0.8025\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4132 - accuracy: 0.7940\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3888 - accuracy: 0.7736\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.7872\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4316 - accuracy: 0.7690\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4141 - accuracy: 0.8055\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3784 - accuracy: 0.8135\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3886 - accuracy: 0.8270\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4179 - accuracy: 0.7923\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4066 - accuracy: 0.7993\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3747 - accuracy: 0.7939\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3892 - accuracy: 0.7990\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4108 - accuracy: 0.7870\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.7856\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4171 - accuracy: 0.8041\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8388\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8250\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3852 - accuracy: 0.8054\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8321\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8133\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3850 - accuracy: 0.8049\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8418\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8433\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3444 - accuracy: 0.8437\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3297 - accuracy: 0.8613\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3801 - accuracy: 0.8245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8379\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3435 - accuracy: 0.8453\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8197\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8422\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3503 - accuracy: 0.8383\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8281\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3735 - accuracy: 0.8242\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3773 - accuracy: 0.8358\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3199 - accuracy: 0.8550\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3850 - accuracy: 0.8226\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3820 - accuracy: 0.8211\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8280\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3618 - accuracy: 0.8282\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.3619 - accuracy: 0.8268\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.80 - 0s 1ms/step - loss: 0.3908 - accuracy: 0.8179\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3438 - accuracy: 0.8433\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.8629\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3305 - accuracy: 0.8646\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8351\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8591\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.8671\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3490 - accuracy: 0.8586\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8655\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8702\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8677\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.8466\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.8870\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8797\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.82 - 0s 1ms/step - loss: 0.3100 - accuracy: 0.8628\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.8824\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000000263ECAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8205\n",
      "[CV] ........... batch_size=40, epochs=100, score=0.821, total=   2.0s\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 821us/step - loss: 0.6387 - accuracy: 0.7046\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.5151 - accuracy: 0.7362\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.4721 - accuracy: 0.7498\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.4126 - accuracy: 0.8090\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.3724 - accuracy: 0.8280\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.3762 - accuracy: 0.8308\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.4214 - accuracy: 0.8155\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.3749 - accuracy: 0.8110\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8171\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.3712 - accuracy: 0.8030\n",
      "Best: 0.7861733198165893, using {'batch_size': 10, 'epochs': 10}\n",
      "0.7861733198165893 (0.08365885451237506) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.765790319442749 (0.08206572462467022) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.7604349255561829 (0.04660245066629213) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.7554690122604371 (0.08171265670118105) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.7631288647651673 (0.07054314220156904) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.7657578706741333 (0.07516002746965326) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.765692949295044 (0.08261478859911855) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.7785134673118591 (0.07610533608943501) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.7758520007133484 (0.057051463768612254) with: {'batch_size': 40, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# Define a random seed\n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Start defining the model\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4, input_dim = 8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    adam = Adam(lr = 0.01)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create the model\n",
    "model = KerasClassifier(build_fn = create_model, verbose = 1)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40]\n",
    "epochs = [10, 50, 100]\n",
    "\n",
    "# make a dictionary of the grid search parameters\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# build and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)\n",
    "grid_results = grid.fit(X_standardized, Y)\n",
    "\n",
    "# summarize the results\n",
    "print(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('{0} ({1}) with: {2}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] neuron1=4, neuron2=2 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000000247CFD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV] ................ neuron1=4, neuron2=2, score=0.835, total=   1.9s\n",
      "[CV] neuron1=4, neuron2=2 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000000025A88670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV] ................ neuron1=4, neuron2=2, score=0.633, total=   1.7s\n",
      "[CV] neuron1=4, neuron2=2 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=2, score=0.821, total=   1.7s\n",
      "[CV] neuron1=4, neuron2=2 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=2, score=0.833, total=   1.6s\n",
      "[CV] neuron1=4, neuron2=2 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    6.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=2, score=0.833, total=   1.8s\n",
      "[CV] neuron1=4, neuron2=4 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=4, score=0.835, total=   1.6s\n",
      "[CV] neuron1=4, neuron2=4 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   10.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=4, score=0.620, total=   1.7s\n",
      "[CV] neuron1=4, neuron2=4 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   12.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=4, score=0.821, total=   1.6s\n",
      "[CV] neuron1=4, neuron2=4 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   13.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=4, score=0.833, total=   1.8s\n",
      "[CV] neuron1=4, neuron2=4 ............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   15.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ neuron1=4, neuron2=4, score=0.833, total=   1.6s\n",
      "[CV] neuron1=4, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=4, neuron2=8, score=0.835, total=   1.6s\n",
      "[CV] neuron1=4, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=4, neuron2=8, score=0.620, total=   1.9s\n",
      "[CV] neuron1=4, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=4, neuron2=8, score=0.821, total=   1.7s\n",
      "[CV] neuron1=4, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=4, neuron2=8, score=0.846, total=   1.7s\n",
      "[CV] neuron1=4, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=4, neuron2=8, score=0.833, total=   1.6s\n",
      "[CV] neuron1=8, neuron2=2 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=2, score=0.835, total=   1.8s\n",
      "[CV] neuron1=8, neuron2=2 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=2, score=0.633, total=   1.8s\n",
      "[CV] neuron1=8, neuron2=2 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=2, score=0.821, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=2 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=2, score=0.833, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=2 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=2, score=0.833, total=   1.9s\n",
      "[CV] neuron1=8, neuron2=4 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=4, score=0.835, total=   1.9s\n",
      "[CV] neuron1=8, neuron2=4 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=4, score=0.620, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=4 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=4, score=0.821, total=   1.9s\n",
      "[CV] neuron1=8, neuron2=4 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=4, score=0.846, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=4 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=4, score=0.833, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=8, score=0.835, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=8, score=0.620, total=   1.8s\n",
      "[CV] neuron1=8, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=8, score=0.821, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=8, score=0.846, total=   1.7s\n",
      "[CV] neuron1=8, neuron2=8 ............................................\n",
      "[CV] ................ neuron1=8, neuron2=8, score=0.833, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=2 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=2, score=0.835, total=   1.8s\n",
      "[CV] neuron1=16, neuron2=2 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=2, score=0.633, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=2 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=2, score=0.821, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=2 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=2, score=0.846, total=   1.6s\n",
      "[CV] neuron1=16, neuron2=2 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=2, score=0.833, total=   1.8s\n",
      "[CV] neuron1=16, neuron2=4 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=4, score=0.835, total=   1.8s\n",
      "[CV] neuron1=16, neuron2=4 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=4, score=0.633, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=4 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=4, score=0.821, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=4 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=4, score=0.846, total=   1.9s\n",
      "[CV] neuron1=16, neuron2=4 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=4, score=0.821, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=8 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=8, score=0.835, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=8 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=8, score=0.633, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=8 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=8, score=0.821, total=   1.9s\n",
      "[CV] neuron1=16, neuron2=8 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=8, score=0.846, total=   1.7s\n",
      "[CV] neuron1=16, neuron2=8 ...........................................\n",
      "[CV] ............... neuron1=16, neuron2=8, score=0.833, total=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7936708807945252, using {'neuron1': 16, 'neuron2': 2}\n",
      "0.7911067724227905 (0.0792746637362651) with: {'neuron1': 4, 'neuron2': 2}\n",
      "0.7885751247406005 (0.08432733392428071) with: {'neuron1': 4, 'neuron2': 4}\n",
      "0.7911392331123352 (0.08583080767748588) with: {'neuron1': 4, 'neuron2': 8}\n",
      "0.7911067724227905 (0.0792746637362651) with: {'neuron1': 8, 'neuron2': 2}\n",
      "0.7911392331123352 (0.08583080767748588) with: {'neuron1': 8, 'neuron2': 4}\n",
      "0.7911392331123352 (0.08583080767748588) with: {'neuron1': 8, 'neuron2': 8}\n",
      "0.7936708807945252 (0.08079181748331918) with: {'neuron1': 16, 'neuron2': 2}\n",
      "0.7911067843437195 (0.07968826468817176) with: {'neuron1': 16, 'neuron2': 4}\n",
      "0.7936708807945252 (0.08079181748331918) with: {'neuron1': 16, 'neuron2': 8}\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "\n",
    "# Define a random seed\n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Start defining the model\n",
    "def create_model(neuron1, neuron2):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1, input_dim = 8, kernel_initializer= 'uniform', activation= 'linear'))\n",
    "    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= 'uniform', activation= 'linear'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    adam = Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create the model\n",
    "model = KerasClassifier(build_fn = create_model, epochs = 100, batch_size = 20, verbose = 0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neuron1 = [4, 8, 16]\n",
    "neuron2 = [2, 4, 8]\n",
    "\n",
    "# make a dictionary of the grid search parameters\n",
    "param_grid = dict(neuron1 = neuron1, neuron2 = neuron2)\n",
    "\n",
    "# build and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), refit = True, verbose = 10)\n",
    "grid_results = grid.fit(X_standardized, Y)\n",
    "\n",
    "# summarize the results\n",
    "print(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\n",
    "means = grid_results.cv_results_['mean_test_score']\n",
    "stds = grid_results.cv_results_['std_test_score']\n",
    "params = grid_results.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print('{0} ({1}) with: {2}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict labels for new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amar\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(X_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate your model and show relevant performance metrics (like precision, recall etc.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7806122448979592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.89      0.84       262\n",
      "         1.0       0.71      0.57      0.63       130\n",
      "\n",
      "    accuracy                           0.78       392\n",
      "   macro avg       0.76      0.73      0.74       392\n",
      "weighted avg       0.77      0.78      0.77       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(accuracy_score(Y, y_pred))\n",
    "print(classification_report(Y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
